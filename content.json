{"pages":[{"title":"about","text":"林哥博客：致力于大数据技术。 联系方式：xiaolin1529@163.com","link":"/about/index.html"},{"title":"林哥博客欢迎您","text":"友链互换请在下方留言 本站名称: 林哥博客 本站头像: https://iochina.top/images/pht.png 本站地址: https://iochina.top 大佬博客 Zhao Blog: ‘https://fastflink.com' Spark 社区: ‘http://sparkfuns.com' 左岸博客: ‘https://www.zrahh.com'","link":"/friends/index.html"}],"posts":[{"title":"ELK全量日志查询浅析","text":"ELK 全量日志查询项目需求由来1. 开发人员不能登录线上服务器查看详细日志，经过运维周转费时费力 2. 日志散落在多个系统上，难以查找和整合。 3. 日志数量巨大，查询速度太慢，难以满足需求。 4. 无法全局掌握项目运行情况 5. 日志数据查询不够实时 6. 数据分析人员不会写代码，无法分析统计数据。 7. ....... ELK全量日志查询 Logstash+ElasticSearch+Kibana（分布式日志系统） Logstash:监控，过滤，收集日志 Elasticsearch：存储日志，提供查询功能。 Kibana： 提供web界面，支持查询统计和图表展现。 filebeat： 轻量级的日志收集工具。 架构设计 使用filebeat filebeat（1.3）–&gt; logstash(parse) –&gt; es –&gt;kibana –ngix 如果logstash出现问题会导致filebeat收集的数据丢失 filebeat(1.3) –&gt; logstash(parse)[loadbalance] –&gt; es –&gt;kibana–mgix filebeat和logstash耦合性太高 filebeat(1.3) –&gt; redis –&gt; logstash(parse)–&gt;es–&gt;kibana–ngix 里面redis是一个单线程的实例，redis单线程每秒处理能力一般是10W次左右，相对于kafka来说不值得一提，遇到数据量特别巨大情况可能会发生问题。 filebeat –&gt;kafka –&gt;logstash(parse)–es–&gt;kibana –ngix kafka可以水平扩展，使用多分区，支持多线程并行执行。 在应用端收集日志的话，logstash比较重量级，性能消耗比filebeat大。 filebeat用于日志收集和传输，相比Logstash更加轻量级和易部署，对系统资源开销更小。 logstash与filebeat来说，支持的数据源更多，且可以对数据进行过滤。 安装部署 filebeat，轻量级日志收集工具，使用go语言开发。 下载 解压 tar 配置filebeat.yml（输入数据源和输出） 常见命令 前台启动： ./filebeat -c filebeat.yml 后台运行命令： nohup ./filebeat -c filebeat.yml &gt;/dev/null 2&gt;&amp;1 &amp; 帮助命令: ./filebeat -h input和output input 通过path指定要监控的数据 https://www.elastic.co/guide/en/beats/filebeat/6.4/configuration-filebeat-options.html output 指定数据输出 elasticsearch logstash redis kafka file console https://www.elastic.co/guide/en/beats/filebeat/6.4/configuring-output.html Logstash 是一个采集日志和事件的工具 安装jdk 解压 tar 使用 bin/logstash -e ‘input { stdin { } } output { stdout {} }’ 测试运行 使用bin/logsatsh -f 配置文件 来运行 具体数据源和数据输出参考官网 elasticsearch 基于lucene的开源搜索引擎，是一个分布式的搜索分析系统。 下载 https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-6.4.3.tar.gz 解压 tar -zxvf elasticsearch-6.4.3.tar.gz 启动 bin/elasticsearch 测试 http://localhost:9200/ kibana 下载 启动 bin/kibana 测试 http://localhost:5601 使用ELK的优点 Logstash 提供了大量插件，通过配置就可以完成各种不同日志处理需求。 有统一的查看界面(kibana),减少代码量。 有助于开发人员分析排查问题。 其他问题 logstash的时区问题，logstash中的@timestamp时间比我们当前时间相差8个小时 原因，默认情况下logstash生成的时间是使用UTC时间，我们是在UTC+8时区。 解决方案：kibana，在kibana显示的时候会读取浏览器的当前时区，然后在页面上转换时间内容的显示。 multiline异常信息整合 原因： 使用filebeat收集日志会出现一个问题，filebeat默认按行进行日志收集，如果出现一条异常信息占多行，就会把异常信息按行存储。 解决方案： 修改filebeat.yml文件 12345mutiline: # 匹配以20开头以后的信息 parttern: ^20 negate: true match: after","link":"/2019/02/26/ELK全量日志查询浅析/"},{"title":"Python基础__执行shell命令","text":"Python 执行shell命令 导入os，commands库 os.system(‘java -version’)返回执行结果。0或其他 (status,output) = commands.getstatusoutput(‘java -version’) 返回状态值和输出值。 与用户交互： python获取用户输入参数 python a.py x y z import sys,os sys.argv[0]:a.py,依次取值。len(sys.argv)判断用户输入个数 os_exit(0)程序退出，以及退出值，在linux用$?获取。 代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243# coding=utf8import os,commands'''python 操作shell命令'''def func(): ''' 返回命令执行的状态码，无法获取命令执行输出结构 :return: ''' res = os.system('java -version') print(res)def func2(): \"\"\" 使用commands也可以执行命令 并且可以获取到命令的执行状态码和执行中输出的结果 status：状态码 output：输出内容 :return: \"\"\" (status,output) = commands.getstatusoutput('java -version') print(output,status)def func3(): # len 可以返回集合中元素的个数 print(len(sys.argv)) # 获取集合中第一个元素 print(sys.argv[0]) # os._exit(1) 结束程序，返回指定的状态码 if len(sys.argv) == 3: print(sys.argv[1]) print(sys.argv[2])if __name__ == '__main__': #func() func2()","link":"/2019/03/27/Python基础-执行shell命令/"},{"title":"Mysql  内连接与外连接","text":"Mysql 内连接与外连接 建表语句 1234567891011CREATE TABLE `a_table` ( `a_id` int(11) DEFAULT NULL, `a_name` varchar(50) DEFAULT NULL, `a_part` varchar(50) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE TABLE `b_table` ( `b_id` int(11) DEFAULT NULL, `b_name` varchar(50) DEFAULT NULL, `b_part` varchar(50) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 a_table数据 12341 张三 化学2 李四 物理3 王五 生物4 赵六 数学 b_table 数据 12342 李四 物理3 王五 生物5 陈七 语文6 钱八 历史 内连接 又叫等值连接，返回两个表中连接字段相等的行 inner join on 1234567#执行语句 select * from a_table a inner join b_table b on a.a_id = b.b_id #执行结果 2 李四 物理 2 李四 物理 3 王五 生物 3 王五 生物#说明#组合两个表中的记录，返回符合关联条件的记录（交集） 左连接 又叫左外连接，返回包括左表的所有记录和右表中连接字段相等的记录。 left join on 12345678#执行语句 select * from a_table a left join b_table b on a.a_id = b.b_id select * from a_table a left outer join b_table b on a.a_id = b.b_id #执行结果 2 李四 物理 2 李四 物理 3 王五 生物 3 王五 生物 1 张三 化学 4 赵六 数学 右连接(右外连接) 返回包括右表中所有记录和左表中连接字段相等的记录 right join/right outer join 12345678#执行语句 select * from a_table a right join b_table b on a.a_id = b.b_id select * from a_table a right outer join b_table b on a.a_id = b.b_id#执行结果 2 李四 物理 2 李四 物理 3 王五 生物 3 王五 生物 5 陈七 语文 6 钱八 历史 全连接（全外连接） 返回表中所有记录和左右表中连接相等的记录 full join on mysql中不支持全连接，可以通过左连接+union+右连接实现 123456789101112#执行语句 #select a.a_id,b.b_id from a_table a full join b_table b on a.a_id = b.b_id select * from a_table a left join b_table b on a.a_id = b.b_id union select * from a_table a right join b_table b on a.a_id = b.b_id #执行结果2 李四 物理 2 李四 物理3 王五 生物 3 王五 生物1 张三 化学 4 赵六 数学 5 陈七 语文 6 钱八 历史","link":"/2019/08/31/Mysql-内连接与外连接/"},{"title":"Python基础— 文件读写模式","text":"Python2.7 文件读写模式简单介绍 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# coding=utf8# python 不换行2.x 句尾加, 3.x 添加end=''def funcFileMode(mode='r',file='G:/test1.txt'): ''' # r 只读 如果文件不存在则报错 mode = 'r' with open(file,mode) as wo: for line in wo: print (line.strip()) ''' ''' # r+ 可读可写 ，若文件不存在，即报错，写入时采用添加 mode = 'r+' with open(file,mode) as wo1: for line in wo1: print (line.strip()) wo1.write(\"new world \") ''' ''' # w 可写，若文件不存在，即报错，写入时采用覆盖 mode = 'w' with open(file,mode) as wo2: wo2.write(\"new world \") ''' ''' # w+ 可写不可读，若文件不存在，即创建，写入时采用覆盖 mode = 'w+' with open(file,mode) as wo3: wo3.write(\"new world \") ''' ''' # a 可写不可读，若文件不存在，即创建，写入时采用追加 mode = 'a' with open(file,mode) as wo4: wo4.write(\"new world \") ''' # a+ 可写可读，若文件不存在，即创建，写入时采用追加 mode = 'a+' with open(file,mode) as wo5: wo5.write(\"new world \") # 设置文件指针指向文件首 wo5.seek(0) print wo5.read()if __name__ == '__main__' : funcFileMode()","link":"/2019/08/27/Python基础—-文件读写模式/"},{"title":"impala","text":"Impala impala是参照谷歌新三篇论文Dremel的开源实现。Impala是Cloudera公司主导开发并开源。基于Hive并使用内存进行计算，兼顾数据仓库，具有实时、批处理、多并发等优点。是使用CDH的首选PB级大数据实时查询(OLAP)分析引擎。有测试表明，Impala的性能较Hive提高了3~90倍。 Impala与hive impala和hive的关系 impala是基于Hive的大数据实时分析查询引擎，直接使用Hive的元数据库Metadata，意味着impala元数据都存储在Hive的metastore中。并且impala兼容Hive的sql解析，实现了Hive的SQL语义的子集，功能还在不断完善中。 直接选用impala代替hive，是因为impala快 基于内存计算，能够对PB级数据进行交互式实时查询、分析。 摒弃了MR计算改用C++编写，有针对性的硬件优化，例如使用SSE指令。 兼容HiveSQL，无缝迁移。 通过使用LLVM来统一编译运行时代码，避免为了支持通用编译而带来的不必要的开销。 支持sql92标准，并且有自己的解析器和优化器。 具有数据仓库的特性，对hive原有数据做数据分析。 使用了支持Data locality的/O调度机制。 支持列式存储。 支持jdbc/odbc远程访问。 impala不足 基于内存进行计算，对内存依赖性较大 改用C++编写，以为着对C++普通用户不可见。 基于Hive，与Hive共存亡。 实践中impala的分区超过一万，性能严重下降，容易出现问题。 稳定性不如Hive Impala架构 三大进程 impalad statestored catalogd impala daemon（服务名称 impalad） -N个实例 接收client、hue、jdbc或odbc请求、query执行返回给中心协调节点。 子节点上的守护进程，负责向statestore保持通信，汇报工作。 Statestore daemon (服务名称statestored) -1个实例 负责收集分布在集群中各个impalad进程的资源信息，各个节点健康状况、同步节点信息 负责query的调度 对于一个正常运转的集群，并不是一个关键进程。 Catalog daemon（服务名称catalogd） - 1个实例 把impala表的metadata分发到各个impalad中 接收来自statestore的所有请求 Impala监控 查看catalog： http://host:25020/ 查看stastore： http://host:25010/","link":"/2019/08/28/impala/"},{"title":"Spark优化","text":"Spark 性能优化分析 Spark的性能优化，主要手段包括 使用高性能序列化类库，如使用kryo来代替java序列化 优化数据结构（算子函数内部使用到的局部数据或者算子函数外部的数据都可以进行数据结构的优化，优化以后，都会减少对内存的消耗和占用。） 对多次使用的RDD进行持久化/Checkpoint，避免后面使用需要重复计算，降低性能 使用序列化的持久化级别 Java虚拟机垃圾回收优化 提高并行度 广播共享数据 数据本地化 reduceByKey和groupByKey的适当使用 Shuffle调优","link":"/2019/01/22/Spark优化/"},{"title":"python基础  接口开发","text":"api 调用实例","link":"/2019/08/27/python基础-接口开发/"},{"title":"大数据框架Hadoop(一)","text":"Hadoop是一个适合海量数据的分布式存储和分布式计算的平台。Hadoop特点 扩容能力：可以横向扩容 成本低：普通机器就可以组件集群 高效率：分发计算到数据节点 可靠性：数据多副本，失败后可自动恢复. Hadoop由三个组件构成（HDFS，MapReduce，Yarn） hdfs：是一个分布式存储框架，适合海量数据存储。 负责数据分布式存储 主从结构 主节点，最多有2个 namenode 从节点，可以有多个 datanode namenode负责 接收用户请求，是用户操作的入口 维护文件系统的目录结构，称作命名空间 datanode负责 存储数据 mapreduce：分布式计算框架，适合海量数据计算。 Mapredue是一个编程模型，它是分布式运行的，由两个阶段组成： Map和Reduce Map阶段是一个独立程序，有很多节点同时运行，每个节点处理一部分数据。 Reduce阶段也是一个独立的程序，在这先把reduce理解为一个单独的聚合程序即可。 yarn：资源调度平台，负责给计算框架分配计算资源。 资源调度和管理平台 主从结构 主节点，最多可以2个：ResourceManager 从节点，有很多个： NodeManager ResourceManager负责 集群资源的分配与调度。 MapReduce、Storm、Spark等应用，必须实现ApplicationMaster接口，才能被RM管理。 NodeManager负责 单节点资源的管理。 Hadoop目前已经有三个大版本 Hadoop1.x 和hadoop2.x架构对比 Hadoop1.x里面的计算任务和资源管理都是由mapreduce完成的，Hadoop2.x把资源管理分离出来设计了Yarn，Mapreduce仅负责计算 Hadoop2.x和Hadoop3.x架构没有多大改变 Hadoop 安装部署部分参数介绍 core-site.xml 123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!-- hdfs的数据接口--&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop100:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- hadoop在节点的临时数据目录--&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop_repo&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456789101112131415&lt;configuration&gt; &lt;property&gt; &lt;!-- 伪分布式只有一个从节点,副本参数只能设置为1--&gt; &lt;!-- 分布式副本参数默认为3--&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!-- HA机制,分布式才可使用secondNameNode--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop100:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 123456789101112131415&lt;configuration&gt; &lt;property&gt; &lt;!-- 自定义shuffle服务 --&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Yarn主节点(ResourceManager)--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop100&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml 1234567&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定资源管理框架--&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; slaves 伪分布式localhost 分布式 12node1Hostnode2Host","link":"/2019/09/02/大数据框架Hadoop-一/"},{"title":"python基础 连接数据库","text":"python 连接数据库 安装mysql-python驱动模块 如果你使用的默认安装的python也可以使用 yum 安装 MySQL-python yum install MySQL-python【不推荐】 如果你要在linux 下开发python程序要安装一下开发包。否则可以忽略。 yum install python-devel mysql-devel zlib-devel openssl-devel windows下直接使用pip install mysql-python，安装容易出问题：安装VCForPython27.msi 和 MySQL-python-1.2.3.win-amd64-py2.7.exe即可 模块引入之后我们就需要和数据库进行连接了，实例代码如下： import MySQLdb db = MySQLdb.connect(“127.0.0.1”,”root”,”admin”,”test” ) db = MySQLdb.connect(host=”127.0.0.1”, user=”root”, passwd=”admin”, db=”test”,charset=”utf8”) 详细参数参看 Connection类。 数据库连接对事务操作的方法：1 commit() 提交 2 rollback() 回滚 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# coding=utf8import MySQLdb'''Python 访问数据库 MySQLdb.connect()连接字段解释 host:数据库主机名，默认本地主机(localhost) user:数据库用户名,默认当前用户 passwod：数据库密码 db: 连接的数据库名 charset:连接字符集 conv: 将文字映射到Python类型的字典，默认为MySQLdb.converters.conversions cursorclass: cursor()使用的种类，默认为MySQL.cursors.Cursor. compress: 启用协议压缩功能。 named_pipe: 在window中，与一个命名管道相连接。 init_command: 一旦连接建立，就为数据库服务器指定一条语句来运行。 read_default_file: 使用指定的MySQL配置文件 read_default_group: 读取的默认组。 unix_socket在unix中，连接使用的套接字，默认使用TCP。 port，指定数据库服务器的连接端口，默认3306'''def func(): # 获取连接 db = MySQLdb.connect(host='localhost', port=3306, user='root', passwd='0000', db='data', charset='utf8') db_cursor = db.cursor() # 执行sql查询 sql = \"select * from news_info limit 5\" db_cursor.execute(sql) # 获取查询结果 lines = db_cursor.fetchall() for line in lines: print(line[1]) # 新增数据 # res = db_cursor.execute(\"insert into news_info (id,title) values('%s','%s');\"%('id2','title2')) # print(res) # 修改数据 #res=db_cursor.execute(\"update news_info set title = '新标题1' where id = '00093f78-11c7-43e1-abed-86c735252155' \") #print (res) # 删除数据 res= db_cursor.execute(\"delete from news_info where id = 'id2'\") print(res) db.commit() db.close()if __name__ == '__main__': func()","link":"/2019/08/27/python基础-连接数据库/"},{"title":"zookeeper介绍","text":"zookeeper Zookeeper是一个分布式协调服务。 分布式应用程序可以基于zookeeper实现同步服务，配置维护和命名服务等。 zookeeper可以保证数据在zookeeper集群之间的数据的事务性一致。 zookeeper的特点 数据一致性：为客户端展示同一视图 可靠性：如果消息被一台服务器接受，那么它将被所有服务器接受 原子性：更新只能成功或失败，没有中间状态。 zookeeper架构​ zookeeper的角色 领导者(leader)，负责进行投票的发起和决议，更新系统状态 学习者（learner），包括跟随者(follower)和观察者(observer)，follower用于接受客户端请求并向客户端返回结果，在选主过程中参与投票。observer可以接受客户端连接，将写请求转发给leader，但observer不参加投票过程，只同步leader的状态，observer的目的是为了扩展系统，提高读取速度。 客户端（client):请求发起方 zookeeper命令行 常用命令 查看节点下面信息 ls / 创建节点 create /test data 获取节点上保存的数据 get /test 修改节点数据 set /test databak 递归删除节点 rmr /test 使用delete命令删除节点，但是节点下面不能有子节点 zookeeper的数据模型 层次化的目录结构，命名符合常规文件系统规范。 每个节点在zookeeper中叫做znode，并且其有一个唯一的路径标识。 节点znode可以包含数据和子节点，但是ephemeral（临时节点）类型的节点不能有子节点 znode中的数据可以有多个版本，比如某一个路径下存有多个数据版本，那么查询这个路径下的数据就需要带上版本。 客户端应用可以在节点上设置监视器。 节点不支持部分读写，而是一次性完整读写。 zookeeper的节点 znode的两种类型，临时节点(ephemeral)和永久节点(persistent) znode的类型在创建时确定并且之后不能再更改 临时znode的客户端会话结束时，zookeeper会将该临时znode删除，临时znode不可以有子节点。 永久znode不依赖于客户端会话，只有当客户端明确要删除该node时，才会被删除。 znode有四种形式的目录节点，persistent、persistent_sequential、ephemeral、ephemeral_sequential. ^(*￣(oo)￣)^： znode可以是临时节点，一旦创建这个znode的客户端与服务器失去联系，这个znode也将自动删除，zookeeper的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态成为session，如果znode是临时节点，这个session失效，znode也就删除了，持久化目录节点这个目录节点存储的数据不会丢失。 zookeeper实现分布式监控 假设要监控多台服务器上的A程序运行状态，当发现有服务器上的A程序下线的时候，给管理员发短信，并尝试重启A程序 主要利用zk的临时节点和watcher监视器特性 首先在A程序启动的时候在zookeeper的/monitor节点下创建一个临时节点的名称，可以用这个服务器的主机名或者ip信息，只要A程序一直正常运行，这个临时节点就会一直存在。 给zk的/monitor节点注册一个watcher监视器，监视monitor节点下面的所有子节点的变化情况，当有子节点变化的时候，获取到具体是哪个子节点发生了变化，就知道哪台机器上的A程序有问题了。 可以给管理员发短信，打电话，发邮件之类的，并且还可以实现对那一台服务器上的A程序进行重启。 zookeeper实现分布式共享锁 首先假设有两个线程，两个线程要同时到mysql中更新一条数据，对数据库中的数据进行累加更新。由于在分布式环境下，这两个线程可能存在于不同的机器上的不同jvm进程中，所以这两个线程的关系就是垮主机跨进程，使用java中的synchronized锁是搞不定的 主要利用了zookeeper的临时有序节点特性和watcher监视器。 我们认为最小的节点具备执行权，也就是获取到了锁。 大致思路 1：当这两个线程去mysql更新数据之前，先到zookeeper的/locks(永久节点)下面注册一个临时有序节点，这样每个线程都注册了一个临时节点，两个临时节点肯定是有序的。 线程1：/locks/000 000 000 2 线程2：/locks/000 000 000 1 当每个线程注册完节点之后，需要尝试获取锁，这个时候，哪个节点最小，哪个节点就获取到锁，这个时候。线程2注册的节点最小，所以线程2就获取到锁，执行更新数据库的代码，更新完成之后，删除自己注册的临时节点。 同时线程1会判断自己是不是最小的，所以会监控比自己小1的那个节点，当发现那个节点消失的话，也就意味着它的节点就是最小的节点，获取锁，执行更新数据的代码。。。 ​","link":"/2019/08/28/zookeeper介绍/"},{"title":"大数据框架Hadoop(三)  HDFS的HA和联邦","text":"hdfs 2的HA机制 HDFS的HA，指的是在一个集群中存在两个NameNode，分别运行在独立的物理节点上。在任何时间点，只有一个NameNode是处于Active状态，另一种是standby状态。Activite NameNode负责所有客户端操作，而standby NameNode用来同步Active NameNode的状态信息来提高快速的故障恢复能力。 为了保证Active NN和Standby NN节点状态同步，即元数据保持一致。除了DataNode需要向两个NN发送block位置信息外，还构建了一组独立的守护进程——JournalNodes，用来同步Edits信息。当Active NN执行任何有关命名空间的修改，它需要持久化到一半以上的JournalNodes上。而StandBy NN负责观察JNs的变化，从JNs读取从Active 发送过来的Edits信息并更新自己的内部空间，一旦ActiveNN遇到错误，StandBy NN需要保证从JNs中读出了全部的Edits，然后切换成Active状态。 使用HA的时候，不能启动SecondNameNode，会出错。 HDFS2 的Federation（联邦） HDFS Federation设计可解决单一命名空间存在的以下几个问题： hdfs集群扩展性。多个NameNode分管一部分目录，使得一个集群可以扩展到更多节点，不再像1.0呢样由于内存的限制制约文件存储数目。 性能更高效。多个NameNode管理不同的数据，而且同时对外提高服务，将为用户提高更高的读写吞吐率。 良好的隔离性。用户可根据需要将不同业务数据交由不同的NameNode管理，这样不同业务至今啊影响很小。 hdfs常见问题 集群启动失败 查看日志 hdfs 文件无法操作 一般是处于安全模式 离开安全模式 hdfs dfsadmin -safemode leave 进入安全模式 hdfs dfsadmin -safemode enter 查看安全模式 hdfs dfsadmin -safemode get","link":"/2019/09/03/大数据框架Hadoop-三-HDFS的HA和联邦/"},{"title":"新一代大数据引Flink(一)——简单介绍","text":"Flink简介 Apache Flink 是一个开源的分布式、高性能、高可用、准确的流处理框架。 支持实时流(Stream)处理和批(Batch)处理,批处理是流处理的一个极限特例。 Flink原生支持迭代计算、内存管理和程序优化。 Flink基本组件 Flink的流处理和批处理详解 在大数据领域，批处理任务和流处理任务一般被认为是两种不同的任务，一个大数据框架一般会被设计为只能处理其中一个任务。 Storm仅支持流处理任务，MapReduce、Spark只支持批处理任务。Spark Streaming是Apache Spark上一个支持流处理的子系统，Spark Streaming采用了一种micro-batch的架构，即把输入的数据流切分成细粒度的batch，并为每一个batch数据提交一个批处理的Spark任务，所有Spark Streaming本质上还是基于Spark批处理系统对流式数据进行处理，和Storm等完全流式的数据处理方式完全不同。 Flink通过灵活的执行引擎，能够同时支持批处理任务与流处理任务。 Flink应用场景分析 优化电商网站的实时搜索结果 阿里巴巴所有基础团队使用Flink实时更新产品细节和库存信息(Blink)。 针对数据分析团队提供实时流处理服务 通过Flink数据分析平台实时提供数据分析服务，即使发现问题。 网络/传感器检测和错误检测 Bouygues电信公司，法国最大电信供应商之一使用Flink监控其有线和无线网络，实现快速故障响应。 商业智能分析ETL Zalando使用Flink转换数据以便加载到数据仓库，将复杂的转换操作转化为相对简单的并确保分析终端用户可以更快的访问数据(实时ETL)","link":"/2019/08/31/新一代大数据引Flink(一)——简单介绍/"},{"title":"大数据框架概念初级整合","text":"Hadoop Hadoop是一个适合海量数据的分布式存储和分布式计算平台。 Hadoop由HDFS，Map Reduce，Yarn三大组件组成。 HDFS是一个分布式文件系统 MapReduce是一个海量数据计算框架 Yarn是一个资源管理和任务调度框架 Hadoop由三大版本 Apache的原版 CDH(收费) HDP（开源 ，Ambari） Hadoop的特点 扩容能力(Scalabel)：能可靠地存储和处理PB级别的数据。如果数据量更大，存储不下了，再增加节点就可以了。 成本低(Economical)：可以通过普通机器组成的服务器集群来分发以及处理数据，这些服务器集群可达数千个节点。 高效率(Efficient):通过分发计算程序Hadoop可以在数据所在节点上（本地）并行地处理他们，这使得处理非常迅速。 可靠性(Reliable):Hadoop能够自动地维护数据的多分副本，并且在任务失败后能够自动地重新部署计算任务。 Hadoop版本 2.7.5 Redis Redis是一种面向”键/值”对数据类型的内存数据库，可以满足我们对海量数据的快速读写需求。 redis的键只能是字符串类型 redis的值支持多种类型 字符串 string 哈希 hash 字符串列表 list 字符串集合 set 不重复，无序 有序集合sorted set，不重复，有序 Redis特点 高性能: (Redis读的速度是11W次/s，写的速度是8.1W次/s) 注意redis是一个单线程的服务 原子性（保证数据的准确性） 持久存储（两种方式RDB/快照，AOF/日志） 主从结构（master-slave，负载均衡，高可用） 支持集群（3.0版本开始支持） Redis版本 3.2.0 Zookeeper Zookeeper是一个分布式协调服务 Zookeeper提供的服务 同步服务 配置维护 命名服务 Zookeeper特性 全局数据一致性：每个zookeeper节点上展示的数据都是一致的。 可靠性： 如果消息被一台服务器所接受，那么它将被所有服务器接受。 原子性：事务要么成功，要么失败，不会局部化 实时性：保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失败的消息。 zookeeper版本：3.4.9 Hive Hive是一个基于Hadoop的数据仓库工具，可以把结构化的数据文件映射为一张数据表，提供类SQL查询功能。 Hive特点 基于Hadoop的大数据计算/扩展能力 支持类SQL查询语言 统一的元数据管理 简单编程 Hive版本：1.2.2 Flume Flume是一个分布式、高可靠、高可用的系统，能够有效收集、聚合、移动大量的日志。 它有一个简单、灵活的基于流的数据流结构。 具有故障转移机制和负载均衡机制。 使用了一个简单的可扩展的数据模型(Source、Channel、Sink)。 Agent是一个持续传输数据的服务 主要组件包含：Source、Channel、Sink 数据在组件传输的单位是Event。 Flume版本：1.8.0 Kafka Kafka是一个消息队列。它具有高吞吐量，持久性，和分布式的特性。 高吞吐量：Kafka消息是基于磁盘顺序读写的，磁盘顺序读写的速度比内存随机读写的速度快。 持久性：Kafka有一套完善的消息存储机制，确保数据的高效安全的持久化。（数据默认存储7天） 分布式：基于分布式的扩展和容错机制：Kafka的数据都会复制到几台服务器上。当某一台失效时，生产者和消费者转而使用其他的机器。 Kafka版本：2.11-0。11.0.3（scala版本是2.11） Sqoop Sqoop 是一款开源工具，主要用于在Hadoop（Hive）与传统的数据库（Mysql。。）进行数据传递，可以将一个关系型数据库中的数据导进到HDFS中，也可以将HDFS的数据导进到关系型数据库中。 导入 mysql–&gt;hdfs mysql–&gt;hive mysql–&gt;hbase 导出 hdfs–&gt;mysql hive–&gt;hdfs–&gt;mysql hbase–&gt;hive–&gt;hdfs Sqoop版本：1.4.7 Hbase Hbase是一个Hadoop DataBase 。Hbase是一个not only sql ，它是键值存储数据库，列式存储数据库。 Hbase特点 高可靠性：依赖于Hadoop的分布式特性和副本机制 高性能：依赖于Hadoop的MapReduce 可伸缩：依赖于Hadoop的分布式特性 面向列:Habse是按列存储的 Hbase版本：1.2.6 Storm Storm是Twitter开源的一个实时数据计算处理框架，能实现高频数据和大规模数据的实时处理。 Storm的组件 Topology：用于封装一个实时计算应用程序的逻辑，类似Hadoop的MapReduce Job Stream消息流：是一个没有边界的Tuple序列，这些Tuples会被以一种分布式的方式并行地创建和处理。 Spouts消息源，是数据生产者，它会从另一个外部源读取数据并向topology里面发出消息：tuple Bolts消息处理者，所有的消息处理逻辑被封装在bolts里面，处理输入的数据流并产生新的输出数据流，可执行过滤，聚合，查询数据库等操作。 Storm的特点： 分布式 容错的 实时处理 流式处理 无数据丢失 Storm版本：1.0.6 Scala Scala是一门多范式的编程语言，类似java编程语言，是实现可伸缩的语言，集成面向对象编程和函数式编程的各种特性。 Scala是基于JVM的一门语言，所有的scala代码都需要经过编译为字节码，安徽交由java虚拟机来运行。 Scala可以任意调用Java代码。 Scala版本：2.11.12 Spark Spark是一种通用的大数据计算框架，期望使用一个技术堆栈完美解决大数据领域的各种计算任务。通用的大数据快速处理引擎。 Spark的组件 Spark Core :用于离线计算 Spark SQL：用于交互式查询 Spark Streaming： 用于实时流计算 Spark MLlib：用于机器学习 Spark GraphX： 用于图计算 Spark特点 速度快：基于内存计算 容易开发上手：基于RDD的计算模型比Hadoop基于MapReduce的计算模型更加易于理解易于上手开发。 超强的通用性：提供了Spark RDD，Spark SQL，Spark Streaming ，Spark MLlib，Spark GraphX等技术组件，可以一站式完成大数据领域的离线批处理、交互式查询、流式计算、机器学习、图计算等常见任务。 集成Hadoop：Spark和Hadoop进行了高度的集成。Hadoop的HDFS、Hbase负责存储，Yarn负责资源调度，Spark负责复杂大数据计算。 Spark版本：2.1.3","link":"/2019/08/28/大数据框架概念初级整合/"},{"title":"数据仓库Hive浅析","text":"数据加载到数据库的两种模式 写模式 数据在加载的时候对数据合法性进行校验，库中的数据是合乎规范的，有利于快速查询和快速处理。 Mysql就是用的这种模式 读模式 在数据读取的时候对数据合法性进行校验，在加载入库的时候不校验。加载速度非常快，适合大量数据的一次性加载。 Hive是读模式。当上传文件到表中，Hive不会对表中的数据进行校验，对于错误的数据格式查询的时候也不会报错，只是会显示为NULL。 数据库和数据仓库的区别 数据库是面向事务处理的。是为捕获数据而设计的。一般存储的是实时的数据。 数据仓库是面向主题设计的。是为分析数据设计的。存储的是历史的数据。将多个不同数据源的数据整理成统一的标准化数据。数据仓库里面的数据相对文档，数据仓库中的数据一般面向决策，主要面对的是查询和更新，修改和删除操作很少。 Hive文件的格式 TextFile hive默认存储格式 存储方式为：行存储 磁盘开销大，数据解析开销大 压缩的text文件hive无法进行合并和拆分 SequenceFile 二进制文件，以k-v形式序列化到文件中 存储方式： 行存储 可以分割，支持压缩 一般选择block压缩 RCFile 存储方式：数据按行分块，按列存储 压缩快，快速列存取 读记录尽量涉及到的block最少 读取需要的列只需要读取灭个row group的头部定义 读取全量的数据操作性能可能比sequencefile没有明显的优势 ORCFile 存储方式： 数据按行分块，按列存储 压缩快，快速列存取 效率比RCFile高，是RCFile的改良版本。 Hive文件压缩格式以及压缩效率 压缩可以减少存储文件所需要的磁盘空间，加速数据在磁盘和网络上的传输。 四种常见压缩算法比较 压缩格式 split native 压缩率 速度 是否Hadoop自带 换成压缩格式后，原来的应用程序是否需要修改 gzip 否 是 很高 快 是 和文本处理一样，不需要修改 lzo 是 是 高 很快 否，需要安装 需要建索引，还要指定输入格式 snappy 否 是 高 很快 否，需要安装 和文本处理一样，不需要修改 bzip2 是 否 最高 慢 是 和文本处理一样，不需要修改 压缩算法效率比较 压缩格式 压缩比 压缩速率 解压速率 gzip 13.4% 21MB/s 118MB/s lzo 20.5% 135MB/s 410MB/s snappy 22.2% 172MB/s 409MB/s bzip2 13.2% 2.4MB/s 9.5MB/s gzip 压缩使用的CPU资源比Snappy或LZO更多，但可提供更好的压缩比，GZIP通常是不常访问的冷数据的不错选择，而Snappy或LZO更加适合经常访问的热数据。 Bzip2还可以为文件提供比gzip2更好的压缩比，但是在压缩和解压时会在一定程度上影响速度。","link":"/2019/01/10/数据仓库Hive浅析/"},{"title":"数据库三大范式浅析","text":"第一范式(1NF):要求数据库表的每一列都是不可分割的原子数据项。 举例说明： 上表中“家庭信息”和“学校信息”均不满足原子性的要求，即不满足第一范式，调整如下 每一列都是不可再分的，符合第一范式 第二范式(2NF): 在第一范式的基础上，要求数据库中每个表中的属性必须完全依赖于主键，而不是主键的一部分（针对联合主键） 举例说明： 上表中，同一个订单中可能包含不同的产品，因此主键必须是“订单号”和“产品号”组成，但是产品数量、产品折扣、产品价格与“订单号”产品号“都相关，但是订单金额和订单时间仅与订单号相关，与产品号无关，这样就不满足第二范式的要求调整如下 第三范式(3NF): 在第二范式基础上，任何非主属性不依赖于其它非主属性，确保数据包中的每一列数据和主键直接相关，而不能间接相关。 举例说明： 上述所有属性完全依赖于学会，满足第二范式，但是班主任姓别和班主任年龄直接依赖的是班主任姓名，而不是主键学号，调整如下 可以把班主任姓名改为工号更为合理","link":"/2019/03/25/数据库三大范式浅析/"},{"title":"大数据框架Hadoop(二)HDFS","text":"hdfs是是一个分布式的（Distributed）文件（File）系统（System） 通俗的说，hdfs是一个可以管理多台机器文件的管理系统。 允许文件通过网络在多台主机上分析的文件系统，可以让多机器上的多用户分享文件和存储空间。 通透性。让实际上通过网络来访问文件的动作，由程序与用户看来，就像是访问本地磁盘一样。 容错。若系统中有些节点宕机，整体来说系统可以持续运作而不会有数据损失（通过副本机制实现） 分布式文件管理系统（hdfs，S3，GFS等）很多，hdfs只是其中一种，hdfs不适合存储小文件。 HDFS Shell hdfs shell是操作分布式文件系统的一个客户端 使用格式 bin/hdfs dfs -xxx URL URL格式是 scheme：//authority/path。HDFS的scheme是hdfs，对本地文件系统来说scheme是file。scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。 如： /parent/child可以表示成hdfs://hadop100:9000/parent/child scheme：//authority/path hdfs：//ip：port/path 大多数hdfs shell的命令和对应Linux shell命令类似 常用命令 -xxx -ls / 路径为空，显示/user/&lt;currentuser&gt; -h 显示文件大小时选用合适单位 -R 递归显示路径的索引内容 -put localSrc hdfsDst 从本地上传文件（文件夹）到hdfs。如果上传位置已经存在同名的文件（文件夹）则报错。 -f 覆盖同名文件（文件夹） -p 上传时保留文件原理的ownid、groupid、accesstime、modification time等属性。 -get hdfsSrc localDst 从hdfs下载文件到本地 -mkdir hdfsDst 创建文件夹 -p 递归创建 -cp hdfsSrc hdfsDst 在hdfs上复制 -help cmd 显示命令帮助 -cat url url 将路径指定文件的内容输出到stdout。 -test -e 判断hdfs目录是否存在 RPC（远程过程调用协议） RPC（Remote Procedure Call）是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。 RPC采用C/S模式。请求程序是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，等待应答信息。在服务器端，进程保持睡眠状态直到调用信息的到达。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最好和互动调用进程接收答复信息，获得进程结构，然后调用执行继续进行。 Hadoop整个架构都是建立在RPC上的。 RPC接口分析 ClientProtocol 客户端（FileSystem）与Namenode通信的接口。 DatanodeProtocol DataNode与NameNode通信的接口。 NamenodeProtocol SecondaryNameNode与NameNode通信的接口。 hdfs体系结构 Client与NameNode、NameNode与SecondNameNode、DataNode与NameNodet通过PRC协议通信。 Client与DataNode通过TCP进行通信。 HDFS 客户端使用 NameNode NameNode是整个文件系统的管理节点。 它接受客户端请求。 维护整个文件系统的目录树，文件/目录的元信息以及每个文件对应的数据块（block）列表。 维护两个关系：文件与block list的关系(对应的关系信息存储在fsimage和edits文件中，当Namnode启动时会把文件中的内容加载到内存中)；DataNode与block的关系（当DataNode启动时，会把当前节点上的block信息和节点信息上报给NameNode） NameNode主要包括以下文件【/data/hadoop_repo/name/current】 fsimage:元数据镜像。存储某一时刻NameNode内存中的元数据信息。 edits：操作日志文件【事务文件】 seen_txid：存放transactionid的文件，format之后为0，代表是Namnode里面edits_*文件的位数，NameNode重启时，会按照seen_txid的顺序,顺序从头跑edits_000 0001 ~到seen_txid的数字。如果根据对应的seen_txid无法加载到对应的文件，NameNode进程将不会完成启动以保护数据一致性。 VERSION：保存了HDFS的版本信息。 SecondaryNameNode 定期把edis文件中的内容合并到fsimage中 这个合并操作成为checkpoint(快照) checkpoint条件：离上一次checkpoint操作是否已经一个小时，或者hdfs已经进行了100万次操作 合并的适合会对edits中的内容进行转化，生成新的内容保存到fsimage文件中。 Hadoop2.x中针对NameNode提供了HA机制，可以有两个NameNode（active NN &amp;&amp; standby NN）。Standby NN 负责实现把edits中的内容合并到fsimage中，同时清除旧的edits文件。所以这种架构下就不需要SecondaryNameNode进程了。 DataNode 提供真实文件数据的存储服务 存储目录通过hdfs-default.xml文件中的dfs.datanode.data.dir参数控制 文件块（block）：最基本的存储单位。对于文件内容而言，一个文件长度大小是size，那么从文件的0偏移开始，按照固定的大小，顺序对文件进行划分并编号，划分好的每一块成一个Block。HDFS默认的block块大小是128M。一个256M的文件会被划分成2个block。 HDFS中，如果一个文件小于一个数据块的大小，并不会占用整个数据块的存储空间。 Replication:多副本，默认是三个 hdfs-site.xml的dfs.replication属性。 Client读取多副本文件过程（取最近节点的block） hdfs 文件读写流程数据存储-读文件 读文件流程分析 调用FileSystem对象的open方法，其实时DistributedFileSystem的实例 DistributedFileSystem通过rpc获得文件的第一个block的location，同一个block按照副本数会返回多个locations，这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面。 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流。客户端调用read方法，DFSInputStream会找出离客户端最近的datanode并进行连接。 数据从datanode源源不断流向客户端。 如果第一块数据读完了，就会关闭指向第一块datanode连接，接着读取下一块，这些操作对客户端来说时透明的，客户端的角度来看只是读一个持续不断的流。 如果第一批block都读完了，DFSInputStream就会去namnode拿下一批blocks的location，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流。 如果在读数据的时候，DFSInputStream和dataNode的通讯发生异常，就会尝试正在读的block的排第二近的datanode，并且会记录哪个datanode发生错误，剩余blocks读的时候会直接跳过该datanode。DFSInputStream也会检查block数据校验和，如果发现一个坏的block，就会先报告到namenode节点，然后DFSInputStream在其他的datanode上读该block的镜像 该设计方向就是客户端直接连接datanode来检索数据并且namenode来负责为每一个block提供最优的datanode，namenode仅仅处理block location的请求，这些信息都加载在namenode的内存中，hdfs通过datanode集群可以承受大量客户端的并发访问。 hdfs 写文件 写文件流程分析 客户端通过调用DistributedFileSystem的create方法创建新文件。 DistributedFileSystem通过RPC调用NameNode去创建一个没有blocks关联的新文件，创建前，NameNode会做各种校验，比如文件是否存在，客户端有无权限去创建等。如果校验通过，NameNode就会记录下新文件，否则就会抛出IO异常。 前两步结束后会返回FSDataOutputStream的对象，和读文件的时候类似，FSDataOutputStream被封装成DFSOutputStream.DFSOutputStream可以协调NameNode和DataNode。客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小packet，然后排成队列data quene. DataStreamer会处理接收data quene，他询问NameNode这个新的Block最适合存储到哪几个DataNode里，比如副本数是3.那么就找到3个最适合的DataNode，把他们排成一个pipeline。DataStreamer把packet按队列输出到管道的第一个DataNode中，第一个DataNode又把packet输出到第二个DataNode中，以此类推。 DFSOutputStream还有一个队列叫ack quene，也是由packet组成，等待DataNode收到响应，当pipeline中所有的DataNode都表示已经收到的时候，这时候ack queue才会把对应的packet包移除掉。 如果在写的过程中某个DataNode发生错误，会采取以下几步： 1. pipeline被关闭掉。 2. 为了防止丢包ack queue里的packet会同步到data queue里。 3. 把产生错误的DataNode上当前在写但未完成的block删掉。 4. block剩下的部分被写到剩下的两个正常的DataNode中。 5. NameNode找到另外的DataNode去创建这个块的复制。当然这些操作对客户端是无感知的。 客户但完成写数据后调用close方法关闭写入流。 DataStreamer把剩余的包都刷到pipeline里然后等待ack信息，收到最后一个ack后，通知DataNode把文件标识为已完成。 注意：客户端执行write操作后，写完得block才是可见的，正在写的block对客户端是不可见的，只有调用sync方法，客户端才确保该文件被写操作已经全部完成，当客户端调用close方法时会默认调用sync方法。是否需要手动调用取决你根据程序需要在数据健壮性和吞吐率之间的权衡。 hdfs 的Trash回收站 Hdfs为每一个用户创建一个回收站目录： /user/用户名/.Trash/, 每个被用户通过Shell删除的文件/目录，在系统回收站中都有一个时间周期，也就是当系统回收站中的文件/目录在一段时间之后没有被用户恢复的话，HDFS就会自动的把这个文件/目录彻底删除，之后用户就永远也找不回这个文件/目录了。 配置：在每个节点（不仅仅是主节点）上添加配置core-site.xml增加内容如下（单位：分钟） 1234&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; 注意：如果删除的文件过大，超过回收站大小的话会提示删除失败。需要指定参数 -skipTrash跳过回收站直接彻底删除。","link":"/2019/09/02/大数据框架Hadoop-二-HDFS/"},{"title":"Spark Streaming(一)","text":"Spark Streaming SparkStreaming是Spark CoreAPI的一种扩展，它可以用于进行大规模、高吞吐量、容错的实时数据流的处理。 支持从很多数据源中读取数据，比如Kafka、Flume、Twitter、ZeroMQ、Kinesis或者TCP Socket。 可以使用类似高阶函数的复杂算法来进行数据处理，比如map、reduce、join、window。处理后的数据可以被保存到文件系统、数据库、Dashboard等存储中。 DStream以及基本工作原理 接收实时输入数据流，然后将数据拆分成多个batch，比如每收集1秒的数据封装为一个batch，然后将每个batch交给Spark的计算引擎进行处理，最后会生产出一个结果数据流，其中的数据，也是由一个一个的batch所组成的。 Spark Streaming提供了一种高级的抽象，叫做DStream（Discretized Stream “离散流”），代表一个持续不断的数据流 DStream可以通过输入数据源来创建比如Kafka、也可以通过对其他DStream应用高阶函数来创建如map、reduce、join、window。 DStream的内部，其实是一系列不断产生的RDD。RDD是Spark Core的核心抽象——不可变的、分布式的数据集。DStream中每个RDD都包含了一个时间段内的数据。 对DStream应用的算子，比如map其实在底层会被翻译成对DStream中每个RDD的操作。比如对一个Dstream执行一个map操作，会产生一个新的DStream。但是，在底层，其实其原理为：对输入DStream中每个时间段的RDD，都应用一边map操作，然后生成新的RDD，即作为新的DStream中的个时间端的一个RDD。底层的RDD的transformation操作，其实，还是由Spark Core的计算引擎来实现的。Spark Streaming对Spark Core进行了一层封装，隐藏了细节，然后对开发人员提供了方便易用的高层次的API。 与Storm的对比分析 对比点 Storm Spark Streaming 实时计算模型 纯实时，来一条处理一条 准实时，对一个时间段内的数据收集起来，作为一个RDD再处理 实时计算延迟度 毫秒级 秒级 吞吐量 低 高 事务机制 支持完善 支持，但不完善 健壮性/容错性 Zoookeeper，Acker，非常强 CheckPoint，WAL，一般 动态调整并行度 支持 不支持 SparkStreaming 与Sorm的优劣分析 两个框架再实时计算领域中都很优秀，只是擅长的领域不同 Spark Streaming仅仅在吞吐量上要比Storm要优秀。但不是所有场景下都注重吞吐量。 Storm在实时延迟度上，比Spark Streaming好多了，前者是纯实时，后者是准实时。而且Storm的事务机制、健壮性/容错性、动态调整并行度等特性，都要比Spark Streaming更加优秀。 但是Spark Streaming，位于Spark生态技术栈中，因此SparkStreaming可以和Spark Core、Spark SQL无缝整合，我们可以实时处理出来的中间数据，立即在程序中无缝进行延迟批操作、交互式查询等操作。这个特点大大增强了Streaming的优势和功能。 Spark Streaming 与Storm的应用场景 对于Storm来说： 建议在需要纯实时，不能忍受1秒以上延迟的场景使用，比如实时金融系统、要求纯实时进行金融交易和分析。 对于实时计算的功能中，要求可靠的事务机制和可靠性机制，即数据的完全精准，一条也不能多不能少，考虑使用storm 如果还需要针对高低峰时间段动态调整计算程序的并行度，以最大限度利用集群资源（小公司，集群资源紧张）。 对一个大数据应用系统，它就是纯粹的实时计算，不需要在中间执行SQL交互式查询、复杂的transformation算子等。 对于Spark Streaming来说 如果对于上述适合Storm的三点（纯实时，事务机制和可靠性、动态调整并行度），一条都不满足的实时场景（不要求纯实时，不要求强大可靠的事务机制、不要求动态调整并行度），可以考虑使用Spark Streaming。 考虑使用Spark Streaming的最主要一个因素，应该对整个项目进行宏观的考虑，即如果一个项目除了实时计算之外，还包括了离线批处理、交互式查询、等业务功能，而且实时计算中可能会牵扯到高延迟批处理、交互式查询等功能，那么就应该首选Spark生态，用Spark Core开发离线批处理，用Spark SQL开发交互式查询，用Spark Streaming开发实时计算，三者可以无缝整合，给系统提供非常高的可扩展性。 StreamingContext详解 有两种创建StreamingContext的方式 第一种 val conf = new SparkConf().setAppName(appName).setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf,Seconds(1)) 第二种 SptreamingContext还可以使用已有的SparkContext来创建 val sc = new SparkContext(conf) val ssc = new StreamingContext(sc,Seconds(1)) appName,是用来在Spark UI上显示的应用名称。master，是一个Spark、Mesos或者Yarn集群的URL或者是local[*] batch interval 可以格局你的应用程序的延迟要求以及可用的集群资源情况来设置。 一个StreamingConext定义以后，必须做以下几件事情： 1通过创建DStream来创建数据源 2通过对DStream定义transformatition和output算子操作，来定义实时计算逻辑 3 调用StreamingContext的start()方法，来开始实时处理数据。 4通过StreamingContext的awaitTermination（）方法，来等待应用程序的终止。 5 也可以通过调用StreamingContext的stop（）方法，来停止应用程序。 需要注意的要点； 1只要一个StreamingContext启动后，就不能再往里面添加计算逻辑，即执行start()方法后，不能再增加任何任何算子了。 2 一个StreamingContext停止之后，是肯定不能够重启的。调用stop（）方法后，不能再调用start() 3一个JVM同时只能有一个StreamingContext启动。在你的应用程序中，不能创建两个StreamingContext。 4 调用stop()方法时，会同时停止内部的SparkContext，如果不希望如此，还希望后面继续使用SparkContext，可以使用stop(false)。 一个SparkContext可以创建多个StreamingContext，只要上一个先用stop(false)停止，再创建下一个即可。 DStream和Receiver详解 DStream 代表了来自数据源的输入数据流。除了文件数据流之外，所有的输入DStream都会绑定一个Receiver对象，该对象是一个关键的组件，用来从数据源接收数据，并将其存储在Spark的内存中，以供后续处理。 Spark Streamig提供了两种内置的数据源支持： 1基础数据源：StreamingContext API中直接提供了对这些数据源的支持，比如文件、socket。 2高级数据源：诸如Kafka、Flume、Kinesis、Twitter等数据源，通过第三方工具类提供支持。这些数据源的使用，需要引用其依赖。 3自定义数据源：我们可以自己定义数据源，来决定如何接受和存储数据。 如果想要在实时计算应用中并行接收多条数据流，可以创建多个输入DStream。这样就会创建多个Receiver，从而并行地接收多个数据流。注意，一个Spark Streaming Application的Executor，是一个长时间运行的任务，因此它会独占分配给Spark Streaming Application的cpu core。从而只要Spark Streaming运行起来以后，它使用的cpu core就没法给其他应用使用了。 使用本地模式运行程序时，绝对不能用local或local[1],因为那样的话，只会给执行输入Dstream的executor分配一个线程。而Spark Streaming底层的原理是，至少要有两条线程，一条线程用来分配Receiver接收数据，一条线程用来处理接收到的数据，因此必须使用local[n]，n&gt;=2的模式。 DStream Kafka数据源 Receiver方式 这种方式使用Receiver来获取数据，Receiver是使用Kafka的高层次Consumer API来实现的。Receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Straming启动的job会去处理那些数据。 默认配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（WAL)。让该机制会同步地接收到的Kafka数据写入分布式文件系统如（HDFS)上的预写日志中。所以，即使底层节点出现了失败，也可以使用日志中的数据进行回复。 注意事项 1此时Kafka中的topic的partition与Spark中的RDD的partition是没有关系的。所以在KafkaUtils.createStream()中，提高partition的数量，只会增加一个Receiver中读取partition的线程的数量。不会增加Spark处理数据的并行度。 2可以创建多个Kafka输入DStream，使用相同的consumer group和topic，来通过多个receiver并行接收数据。 3 如果基于容错的文件系统，比如HDFS，启用了预写日志机制，接收到的数据会被复制到预写日志中。因此，在KafkaUtils。createStream()中，设置的持久化级别是StorageLevel.MEMORY_AND_DISJ_SER. DStream Kafka数据源 Direct方式 Spark1.3引入了不基于Receiver的直接方式，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。 Driect优点 1简化并行读取：如果要读取多个partition，不需要创建多个输入DStream然后对他们进行union操作。Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从Kafka中读取数据。所以在Kafka partition和RDD partition之间，有一个一对一的映射关系 2高性能：如果要保证零数据丢失，在基于receiver的方式中，需要开启WAL机制。这种方式其实效率低下，因为数据实际被复制了两份，Kafka自身就有高可靠的机制，只要Kafka中做了数据的复制，那么就可以通过Kafka的副本进行恢复。 3 一次且仅一次的事务机制： 基于receiver的方式，是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的。这是消费Kafka数据的传统方式。这种方式配合着WAL机制可以保证数据零丢失的高可靠性，但是无法保证数据被处理一次且仅一次，可能会处理两次。因为Spark和ZooKepper之间是不同步的。 基于Direct的方式，是使用Kafka的简单api，Spark Streaming自己就负责追踪消费offset，并且保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。 DStream的transform操作概览 Transformation Meaning map 对传入的每个元素，返回一个 flatMap 对传入的每个元素，返回一个或多个元素 filter 对传入的元素返回true或false，返回false的元素被过滤掉 union 将两个DStream进行合并 count 返回元素的个数 reduce 对所有values进行聚合 countByValue 对元素按照值进行分组，对每个组进行技术，最后返回&lt;K,V&gt;的格式 reduceBYKey 对key对应的values进行聚合 cogroup 对两个Dstream进行连接操作，一个key连接起来的两个RDD的数据，都会以Iterable的形式，出现在一个Tuple中 join 对两个Dstream进行join操作，每个连接起来的pair，作为DStream的RDD的一个元素 transform 对数据进行转换操作【当DStream的API不够用的时候使用这个函数把DStream转成RDD来操作，最后再返回一个新的RDD】 updateStateByKey 为每个key维护一份state，并进行更新 window 对滑动窗口数据执行操作（实时计算中最有特点的一种操作） DStream的transformation操作​ Output Meaning print 打印每个batch中前10个元素，主要用于测试，或者是不需要执行什么output操作时，用于简单触发一些job。 saveAsTextFile(prefix,[suffix]) 将每个batch的数据保存到文件中。每个batch的文件命名格式为：prefix-TIME_IN_MS[.suffix] saveAsObjectFile 同上，但是将每个batch的数据以序列化对象的方式，保存到SequenceFile中。 saveHadoopFile 同上，将数据保存到Hadoop文件中。 foreachRDD 最常用的output操作，遍历DStream中的每个产生的RDD，进行处理。可以将每个RDD中的数据写入外部存储，比如文件、数据库、缓存等。通常在其中是针对RDD执行action操作的，比如foreach。【和transform实现的功能类似，都可以操作RDD，区别是transform有返回值，foreachRDD没有返回值】 output操作 DStream中所有的计算，都是由output操作触发的，比如print().如果没有任何output操作，那么压根就不会执行定义的计算逻辑。 使用foreachRDD outprint操作，也必须在里面对RDD执行action操作，才能触发对每一个batch的计算逻辑。否则，光影foreachRDD output操作，在里面没有对RDD执行action操作，也不会触发任何逻辑。 ForeachRDD 详解 通常在foreachRDD中，就会创建一个Conection，比如JDBC Connection，然后通过Connection将数据写入外部存储。 误区1：在RDD的foreach外部，创建Connection 它会导致Connection对象被序列化后传输到每个Task中。而这种Connection对象，实际上一般是不支持序列化的，也就无法被传输。 误区2：在RDD的foreach操作内部，创建Connection 这种方式是可以的，但是效率低下。因为它会导致对于RDD的每一条数据，都创建一个Connection对象，通常来说，Connection的创建是很消耗性能的。 合理方式1：使用RDD的foreachPartition操作，并且在该操作内部，创建Connection对象，这样就相当于是，为RDD的每个partition创建一个Connection对象，节省资源的多了。 合理方式二：自己手动封装一个静态连接池，使用RDD的foreachPartition操作，并且在该操作内部，从静态连接池中，通过静态方法，获取到一个连接，使用之后再还回去。这样的话，甚至在多个RDD的partition之间，也可以复用连接了。而且可以让连接池采取懒创建的策略，并且空闲一段时间后，将其释放掉。 window滑动窗口操作​ Transform 意义 window 对每个滑动窗口的数据执行自定义的计算。 countByWindow 对每个滑动窗口的数据执行count操作。 reduceByWindow 对每个滑动窗口的数据执行reduce操作。 reduceByKeyAndWindow 对每个滑动窗口的数据执行reduceByKey操作。 countByValueAndWindow 对每个滑动窗口的数据执行countByValue操作。 缓存与持久化机制+checkpoint机制容错机制和事务语义详解性能调优","link":"/2019/01/20/Spark-Streaming-一/"},{"title":"Spark Streaming(二)","text":"缓存与持久化机制+checkpoint机制 Spark Streaming数据流中的数据持久化到内存中 对DStream调用persist()方法，就可以让Spark Streaming自动将该数据流中的所有产生的RDD都持久化到内存中。 如果要对一个Dstream多次执行操作，那么对DStream持久化是非常有用的。因为多次操作可以共享使用内存中的一份缓存数据。 对于基于窗口的操作，比如reduceByWindow、reduceByKeyAndWindow,以及基于状态的操作，比如updateStateByKey，默认就隐式开启了持久化机制。即Spark Streaming默认就会将基于窗口创建的DStrean中的数据缓存到内存中，不需要我们手动调用persist()方法。 对于通过网络接收数据的输入流，比如socket、Kafka等，默认的持久化级别，是将数据复制一份，以便容错。类似于MEMORY_ONLY_SER_2. 与RDD不同的是，DStream中默认的持久化级别，统一都是要序列化的。 CheckPoint机制概述1 实时计算程序的特点就是持续不断的对数据进行计算 对实时计算应用的要求是必须要能够对与应用程序逻辑无关的失败，进行容错。 要实现要求，Spark Streaming程序必须将足够的信息checkpoint到容错的存储系统上，从而让它从失败中进行回复。有两种数据需要被进行checkpoint： 1 元信息checkpoint——定义了流式计算逻辑的信息，保存到容错的存储系统上，比如HDFS。当运行Spark Streaming应用程序的Driver进程所在节点失败时，该信息可用于进行恢复。元信息包括了： 1配置信息——创建Spark Streaming应用程序的配置信息，比如SparkConf中的信息。 2DStream的操作信息——定义了Spark Streaming应用程序的计算逻辑的Dstream操作信息。 3未处理的batch信息——那些job正在排队，还没处理的batch信息。 2 数据checkpoint——将实时计算过程中产生的RDD的数据保存到可靠的存储系统中。 对于一些将多个batch数据进行聚合的，有状态的transformation操作，这是非常有用的。在这种transformation操作中，生成的RDD是依赖于之前的的RDD的，这会导致随着时间的推移，RDD的依赖链条变得越来越长。 依赖链条越来越长，导致失败恢复时间越来越长。可以将有状态的transformation操作执行过程中产生的RDD，定期地被checkpoint到可靠的存储系统上，比如HDFS，从而削减RDD的依赖链条，进而缩短失败恢复时，RDD的恢复时间。 元数据checkpoint主要是为了从driver失败中进行恢复；而RDD checkpoint主要是为了使用到有状态的transformation操作时，能够在其生产出的数据丢失时，进行快速的失败恢复。 何时启用CheckPoint机制 1使用了有状态的transformation操作——比如updateStateByKey，或者reduceByKeyAndWindow操作被使用了，那么checkpoint目录要求是必须提供的，也就是说必须开启checkpoint机制，从而进行周期性的RDD checkpoint。 2 要保证可以从Driver失败中进行恢复——元数据checkpoint需要启用来进行这种情况的恢复。 3并不是所有的Spark Streaming应用程序都需要启用checkpoint机制，如果即不强制要求从Driver失败中自动进行恢复，又没使用有状态的transformation操作，那么就不需要启用checkpoint。事实上，这么做反而是有助于提升性能的。 如何启用CheckPoint机制 1对于有状态的transformation操作，启用checkpoint机制，定期将其生产的RDD数据checkpoint，是比较简单的。 可以通过配置一个容错的、可靠的文件系统(比如HDFS)的目录,来进行checkpoint机制，checkpoint数据就会写入该目录。使用StreamingContext的checkpoint()方法即可。 如果为了要从Driver失败中进行恢复，那么启用checkpoint机制是比较复杂的。需要改写Spark Streaming应用程序。 当应用程序第一次启动的时候，需要创建一个新的StreamingContext，并且调用其start()方法，进行启动。当Driver从失败中恢复过来时，需要从checkpoint目录中记录的元数据中恢复出来一个StreamingContext。 123456789def functionToCreateContext():StreamingContext={ val ssc= new StreamingContext(...) ssc.checkpoint(checkpointDirectory) ... ssc}val ssc= StreamingContext.getOrCreate(checkpointDirectory,functionToCreateContext _)ssc.start()ssc.awaitTermiation() 配置spark-submit提交参数 进行Spark Streaming应用程序的重写后，当第一次运行程序时，如果发现checkpoint目录不存在，那么就使用定义的函数来第一次创建一个StreamingContext，并将其元数据写入checkpoint目录；当从Driver失败中恢复过来时，发现checkpoint目录以及存在了，那么会使用该目录中的元数据创建一个StreamingContext。 上面的重写时实现Driver失败自动恢复的第一步。第二步是必须确保Driver在失败时，自动被重启。 要能够自动从Driver失败中恢复过来，运行Spark Streaming应用程序的集群，就必须及那块Driver运行的过程，并且在它失败时将它重启。对于Spark自身的standalone模式，就需要进行一些配置去supervise driver，在它失败时将其重启。 首先，要在spark-submit中，添加–deploy-mode参数，默认其值为client，即在提交应用的机器上启动Driver；但是，要能够自动重启Driver，就必须将其值设置为cluster；此外需要添加supervise。 使用上述步骤提交应用之后，就可以让driver在失败时自动被重启，并且通过checkpoint目录的元数据恢复StreamingContext。 CheckPoint说明 将RDD checkpoint到可靠的存储系统上，会耗费很多性能。当RDD被checkpoint时，会导致这些batch的处理时间增加。因此，checkpoint的间隔需要谨慎的设置。对于那些间隔很多的batch，比如1秒，如果还有执行checkpoint操作，则会大幅度削减吞吐量。而另外一方面如果checkpoint操作执行的太不频繁，就会当值RDD的lineage变长，又会有失败恢复时y间过长的风险。 对于那些要求checkpoint的有状态的transformation操作，默认的checkpoint间隔通常是batch间隔的数倍，至少是10秒。使用DStream的checkpoint()方法，可以设置这个DStream的checkpoint的间隔时长。通常来说，将checkpoint间隔设置为窗口操作的滑动间隔的5~10倍，是个不错的选择。 程序部署、升级和监控 部署应用程序 1 有一个集群资源管理器，比如standalone模式下的Spark集群，Yarn模式下的Yarn集群等。 2 打包应用程序为一个jar包。 3为executor配置充足的内存，因为Receiver接收到的数据，是要存储在Executor的内存中的，所以Executor必须配置足够的内存来保存接受到的数据。要注意的是，如果你执行的窗口长度为10分钟的窗口操作，那么Executor的内存资源就必须足够保存10分钟内的数据，因此内存的资源要求是取决你执行的操作的 4 配置checkpoint，如果你的应用程序要求checkpoint操作那么就必须配置一个Hadoop兼容的文件系统比如（HDFS）的目录作为checkpoint目录。 5 配置dirver的自动恢复，如果要让driver能够在失败时自动恢复，一方面要重写driver程序，另一方面要在spark-submit中添加参数。 部署应用程序：启用日志预写系统 预写日志机制，简写为WAL，全称为Write Ahead Log。从Spark 1.2版本开始，就引入了基于容错的文件系统的WAL机制。如果启用该机制，Receiver接受到的所有数据都会被写入配置的checkpoint目录中的预写日志。这种机制可以让driver在恢复的时候，避免数据丢失，并且可以确保整个实时计算过程中，零数据丢失。 要配置该机制，首先调用SteamingContext的checkpoint()方法设置一个checkpoint目录。然后需要将spark.streaming.receiver.writeAheadLog.enable参数设置为true。 然而，这种极强的可靠性机制会导致Receiver的吞吐量大幅度下降，因为单位时间内，有相当一部分时间需要将数据写入预写日志。如果又希望开启预写日志机制，确保数据零丢失，又不影响系统的吞吐量，那么可以创建多个输入DStream，启动多个Receiver。 此外，在启用了预写日志机制之后，推荐将复制持久化机制禁用掉，因为所有数据以及保存在容错的文件系统中了，不需要再用复制机制进行持久化保存一份副本了。只要将输入DStream的持久化机制设置一下即可，persist(StorageLevel.MEMORY_AND_DISK_SER). 升级应用程序 两种方式 1 升级后的Spark应用程序直接启动，与旧的Spark应用程序并行执行，当确保新的应用程序启动没问题之后，就可以将旧的应用程序给停掉。但是要注意的是，这种方式只适用于，能够允许多个客户端读取各自独立的数据，也就是读取相同的数据。 2小心地关闭已经在运行的应用程序，使用StreamingContext的stop()方法，可以确保接收到的数据都处理完之后才停止。然后将升级后的程序部署上去，启动。这样，就可以确保中间没有数据丢失和未处理。因为新的应用程序会从老的应用程序未消费到的地方，继续消费。但是注意，这种方式必须是支持数据缓存的数据源才可以，比如Kafka等。如果数据源不支持数据缓存，那么会导致数据丢失。 注意：配置了driver自动恢复机制时，如果想要根据旧的应用程序的checkpoint信息，启动新的应用程序，是不可行的。需要让新的应用程序针对新的checkpoint目录启动，或者删除之前的checkpoint目录。 监控应用程序 当Spark Streaming应用启动时，Spark Web UI会显示一个独立的streaming tab，会显示Receiver的信息，比如是否活跃，接收到了多少数据，是否有异常等；还会显示完成batch的信息，batch的处理时间、队列延迟等。这些信息可以用于监控spark streaming应用的进度。 Spark UI中一下两个统计指标格外重要 1、处理时间——每个batch的数据的处理的耗时 2、调度延时——一个batch在独队列中阻塞住，等待上一个batch完成物理的时间。 如果batch的处理时间，比batch的间隔时间要长，而且调度延迟时间持续增长，应用程序不足以使用当前设定的速率来处理接收到的数据，此时可以考虑增加batch的间隔时间。 容错机制和事务语义详解容错机制的背景 要理解Spark Streaming提供的容错机制，先回忆一下Spark RDD的基础容错语义： 1、RDD，Ressilient Distributed Dataset，是不可变的、确定的、可重新计算的、分布式的数据集。每个RDD都会记住确定好的计算操作的血缘关系，（val lines = sc.textFile(hdfs file); val words = lines.flatMap(); val pairs = words.map(); val wordCounts = pairs.reduceByKey()）这些操作应用在一个容错的数据集上来创建RDD。 2、如果因为某个Worker节点的失败（挂掉、进程终止、进程内部报错），导致RDD的某个partition数据丢失了，那么那个partition可以通过对原始的容错数据集应用操作血缘，来重新计算出来。 3、所有的RDD transformation操作都是确定的，最后一个被转换出来的RDD的数据，一定是不会因为Spark集群的失败而丢失的。 Spark操作的通常是容错文件系统中的数据，比如HDFS。因此，所有通过容错数据生成的RDD也是容错的。然而，对于Spark Streaming来说，这却行不通，因为在大多数情况下，数据都是通过网络接收的。要让Spark Streaming程序中，所有生成的RDD，都达到与普通Spark程序的RDD相同的容错性，接收到的数据必须被复制到多个Worker节点上的Executor内存中，默认的复制因子是2。 基于上述理论，在出现失败的事件时，有两种数据需要被恢复： 1、数据接收到了，并且已经复制过——这种数据在一个Worker节点挂掉时，是可以继续存活的，因为在其他Worker节点上，还有它的一份副本。 2、数据接收到了，但是正在缓存中，等待复制的——因为还没有复制该数据，因此恢复它的唯一办法就是重新从数据源获取一份。 此外，还有两种失败是我们需要考虑的： 1、Worker节点的失败——任何一个运行了Executor的Worker节点的挂掉，都会导致该节点上所有在内存中的数据都丢失。如果有Receiver运行在该Worker节点上的Executor中，那么缓存的，待复制的数据，都会丢失。 2、Driver节点的失败——如果运行Spark Streaming应用程序的Driver节点失败了，那么显然SparkContext会丢失，那么该Application的所有Executor的数据都会丢失。 Spark Streaming容错语义的定义 流式计算系统的容错语义，通常是以一条记录能够被处理多少次来衡量的。有三种类型的语义可以提供： 1、最多一次：每条记录可能会被处理一次，或者根本就不会被处理。可能有数据丢失。 2、至少一次：每条记录会被处理一次或多次，这种语义比最多一次要更强，因为它确保零数据丢失。但是可能会导致记录被重复处理几次。 3、一次且仅一次：每条记录只会被处理一次——没有数据会丢失，并且没有数据会处理多次。这是最强的一种容错语义。 Spark Streaming的基础容错语义 在Spark Streaming中，处理数据都有三个步骤： 1、接收数据：使用Direct方式接收数据。 2、计算数据：使用DStream的transformation操作对数据进行计算和处理。 3、推送数据：最后计算出来的数据会被推送到外部系统，比如文件系统、数据库等。 如果应用程序要求必须有一次且仅一次的语义，那么上述三个步骤都必须提供一次且仅一次的语义。每条数据都得保证，只能接收一次、只能计算一次、只能推送一次。Spark Streaming中实现这些语义的步骤如下： 1、接收数据：不同的数据源提供不同的语义保障。 2、计算数据：所有接收到的数据一定只会被计算一次，这是基于RDD的基础语义所保障的。即使有失败，只要接收到的数据还是可访问的，最后计算出来的数据一定是相同的。 3、推送数据：output操作默认能确保至少一次的语义，因为它依赖于output操作的类型，以及底层系统的语义支持（比如是否有事务支持等），用户可以实现它们自己的事务机制来确保一次且仅一次的语义。 接收数据的容错语义 1、基于文件的数据源 如果所有的输入数据都在一个容错的文件系统中，比如HDFS，Spark Streaming一定可以从失败进行恢复，并且处理所有数据。这就提供了一次且仅一次的语义，意味着所有的数据只会处理一次。 2、基于Direct的数据源 Kafka Direct API，可以保证，所有从Kafka接收到的数据，都是一次且仅一次。基于该语义保障，如果自己再实现一次且仅一次语义的output操作，那么就可以获得整个Spark Streaming应用程序的一次且仅一次的语义 输出数据的容错语义 output操作，比如foreachRDD，可以提供至少一次的语义。那意味着，当Worker节点失败时，转换后的数据可能会被写入外部系统一次或多次。对于写入文件系统来说，这还是可以接受的，因为会覆盖数据。但是要真正获得一次且仅一次的语义，有两个方法： 1、幂等更新：多次写操作，都是写相同的数据，例如saveAs系列方法，总是写入相同的数据。 2、事务更新：所有的操作都应该做成事务的，从而让写入操作执行一次且仅一次。给每个batch的数据都赋予一个唯一的标识，然后更新的时候判定，如果数据库中还没有该唯一标识，那么就更新，如果有唯一标识，那么就不更新。dstream.foreachRDD { (rdd, time) =&gt; rdd.foreachPartition { partitionIterator =&gt;val partitionId = TaskContext.get.partitionId() val uniqueId = generateUniqueId(time.milliseconds, partitionId) // partitionId和foreachRDD传入的时间，可以构成一个唯一的标识 }} 性能调优数据接收并行调优 显式地对输入数据流进行重分区。使用inputStream.repartition()即可。这样就可以将接收到的batch，分布到指定数量的机器上，然后再进行进一步的操作。 什么时候会用 针对分区过少的时候，可以提高分区数据，例如 distinct之后，可以通过repartition增加分区数量 针对分区过多的时候，可以减少分区数据量，例如fliter之后，每个分区的数据量都变少了，所以可以合并分区，也是通过reparation重新指定分区数量即可 数据处理并行度调优 如果在计算的任何stage中使用的并行task的数量没有足够多，那么集群资源是无法被充分利用的。举例来说，对于分布式的reduce操作，比如reduceByKey和reduceByKeyAndWindow，默认的并行task的数量是由spark.default.parallelism参数决定的。你可以在reduceByKey等操作中，传入第二个参数，手动指定该操作的并行度，也可以调节全局的spark.default.parallelism参数。 数据序列化调优 数据序列化造成的系统开销可以由序列化格式的优化来减小。在流式计算的场景下，有两种类型的数据需要序列化。 1、输入数据：默认情况下，接收到的输入数据，是存储在Executor的内存中的，使用的持久化级别是StorageLevel.MEMORY_AND_DISK_SER_2。这意味着，数据被序列化为字节从而减小GC开销，并且会复制以进行executor失败的容错。因此，数据首先会存储在内存中，然后在内存不足时会溢写到磁盘上，从而为流式计算来保存所有需要的数据。这里的序列化有明显的性能开销——Receiver必须反序列化从网络接收到的数据，然后再使用Spark的序列化格式序列化数据。 2、流式计算操作生成的持久化RDD：流式计算操作生成的持久化RDD，可能会持久化到内存中。例如，窗口操作默认就会将数据持久化在内存中，因为这些数据后面可能会在多个窗口中被使用，并被处理多次。然而，不像Spark Core的默认持久化级别，StorageLevel.MEMORY_ONLY，流式计算操作生成的RDD的默认持久化级别是StorageLevel.MEMORY_ONLY_SER ，默认就会减小GC开销。 在上述的场景中，使用Kryo序列化类库可以减小CPU和内存的性能开销。使用Kryo时，一定要考虑注册自定义的类，并且禁用对应引用的tracking（spark.kryo.referenceTracking）。 跟踪对同一个对象的引用情况，这对发现有循环引用或同一对象有多个副本的情况是很有用的。设置为false可以提高性能 在一些特殊的场景中，比如需要为流式应用保持的数据总量并不是很多，也许可以将数据以非序列化的方式进行持久化，从而减少序列化和反序列化的CPU开销，而且又不会有太昂贵的GC开销。举例来说，如果你数秒的batch interval，并且没有使用window操作，那么你可以考虑通过显式地设置持久化级别，来禁止持久化时对数据进行序列化。这样就可以减少用于序列化和反序列化的CPU性能开销，并且不用承担太多的GC开销。 batch interval调优 如果想让一个运行在集群上的Spark Streaming应用程序可以稳定，它就必须尽可能快地处理接收到的数据。换句话说，batch应该在生成之后，就尽可能快地处理掉。对于一个应用来说，这个是不是一个问题，可以通过观察Spark UI上的batch处理时间来定。batch处理时间必须小于batch interval时间。 基于流式计算的本质，batch interval对于，在固定集群资源条件下，应用能保持的数据接收速率，会有巨大的影响。例如，在WordCount例子中，对于一个特定的数据接收速率，应用业务可以保证每2秒打印一次单词计数，而不是每500ms。因此batch interval需要被设置得，让预期的数据接收速率可以在生产环境中保持住。 为你的应用计算正确的batch大小的比较好的方法，是在一个很保守的batch interval，比如5~10s，以很慢的数据接收速率进行测试。要检查应用是否跟得上这个数据速率，可以检查每个batch的处理时间的延迟，如果处理时间与batch interval基本吻合，那么应用就是稳定的。否则，如果batch调度的延迟持续增长，那么就意味应用无法跟得上这个速率，也就是不稳定的。因此你要想有一个稳定的配置，可以尝试提升数据处理的速度，或者增加batch interval。记住，由于临时性的数据增长导致的暂时的延迟增长，是合理的，只要延迟情况可以在短时间内恢复即可。","link":"/2019/01/22/Spark-Streaming-二/"},{"title":"大数据框架Hadoop(四)-MapReduce1","text":"MapReduce概述 MapReduce是一种分布式计算模型，由Google提出，主要用于搜索领域，解决海量数据的计算问题。 MapReduce是分布式运行的，由两个阶段组成：Map和Reduce，Map阶段是一个独立程序，有很多节点同时运行，每个节点处理一部分数据。Reduce阶段是一个独立的程序，有很多节点同时运行，每个节点处理一部分数据。 MapReduce框架都有默认实现，用户只需要覆盖map（）和reduce()两个函数，即可实现分布式计算。两个函数的形参和返回值都是&lt;key,value&gt; MR执行过程-map阶段 map任务处理 框架使用InputFormat类的子类把输入文件（夹）划分为很多InputSplit，默认每个HDFS的block对应一个InputSplit。通过RecordReader类，把每一个InputSplit解析成一个个&lt;k1,v1&gt;。默认框架对每一个InputSplit中的每一行，解析成一个&lt;k1,v1&gt;。 框架调用Mapper类中的map（…）函数，map函数的形参是&lt;k1,v1&gt;对，输出是&lt;k2,v2&gt;对。一个InputSplit对应一个map task。我们可以覆盖map函数，实现自己的逻辑。 （假设reduce存在）框架对map输出的&lt;k2,v2&gt;进行分区。不同的分区中的&lt;k2,v2&gt;由不同的reduce task处理。默认只有1个分区。 （假设reduce不存在）框架对map结果直接输出到HDFS中。 （假设reduce存在）框架对每个分区中的数据，按照k2进行排序分组。分组指的是相同k2的v2分成一组。分组不会减少&lt;k2,v2&gt;的数量。 （假设reduce存在，可选）在map节点，框架可以执行reduce规约(combine) (假设reduce存在)框架会把map task输出的&lt;k2,v2&gt;写入到Linux的磁盘文件中。 至此，map阶段结束。 reduce阶段 框架对多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点。这个过程称作shuffle。 框架对reduce端接收的map任务输出的相同分区的&lt;k2,v2&gt;数据进行合并、排序、分组。 框架调用Reducer类中的reduce方法，reduce方法的形参是&lt;k2,{v2,…}&gt;，输出是&lt;k3,v3&gt;。一个&lt;k2,{v2,…}调用一次reduce函数。我们可以覆盖reduce函数，实现自己的逻辑。 把reduce的输出保存到HDFS中。 至此，整个reduce阶段结束。 shuffle过程 MapReduce执行流程总结 Map Task 读取：框架调用InputFormat类的子类读取HDFS中文件数据，把文件转换为InputSplit。默认，文件的一个block对应一个InputSplit，一个InputSplit对应一个map task。一个InputSplit中的数据会被RecordReader解析成&lt;k1,v1&gt;。默认，InputSplit中的一行解析成一个&lt;k1,v1&gt;。默认v1表示一行的内容，k1表示偏移量。 map：框架调用Mapper类中的(k1,v1)方法，接收&lt;k1,v1&gt;，输出&lt;k2,v2&gt;。有多少个&lt;k1,v1&gt;,map会被执行多少次。我们可以覆盖map(),实现自己的业务逻辑。 分区：框架对map的输出进行分区。分区的目的是确定哪些&lt;k2,v2&gt;进入哪个reduce task。默认只有一个分区。 排序分组：框架对不同分区中的&lt;k2,v2&gt;进行排序、分组。 排序是按照k2进行排序。 分组是指相同k2的v2分到一个组中。 combiner：可以在map task中对&lt;K2,v2&gt;执行reduce规约。 写入本地：框架对map的输出写入到Linux本地磁盘。 Reduce Task shuffle：框架根据map不同分区，将数据，通过网络copy到不同的reduce节点。 合并排序分组：每个reduce会把多个map传来的&lt;k2,v2&gt;进行合并、排序、分组。 reduce：框架调用reduce(k2,v2s).有多少个分组，就会执行多少次reduce函数。 写入HDFS：框架对reduce的输出写入到HDFS中。 查看MapReduce任务输出日志 historyserver进程作用 把收集散落在弄得manager节点上的日志 停止Yarn上的任务 yarn application -kill 在shell中ctrl+c无法停止程序，因为程序已经提交到yarn集群运行了。 yarn application -kill 不仅可以停止mr任务，只要是在yarn上运行的任务都可以使用这个命令杀掉进程。 MapReduce在Yarn上执行流程 首先，Resource Manager会为每一个application（比如一个用户提交的MapReduce Job）在NodeManager里面申请一个container，然后在该container里面启动一个Application Master。container在Yarn中是分配资源的容器（内存、cpu、硬盘等），它启动时便会相应启动一个JVM。 然后，Aplication Master便陆续为application包含的每一个task（一个Map Task或Reduce task）向Resource Manager申请一个container。等每得到一个container后，便要求该container所属的NodeManager将此container启动，然后就在这个container里面执行相应的task。 等这个task执行完后，这个container便会被NodeManager收回，而container所拥有的JVM也相应地被退出。 Yarn核心组件功能 Yarn Client Yarn Client提交Application到RM，它会首先创建一个Application上下文对象，并设置AM必须的资源请求信息，然后提交到RM。Yarn client也可以与RM通信，获取一个已经提交并运行的Application的状态信息等。 ResourceManager（RM） RM是Yarn集群的Master，负责管理整个集群的资源和资源分配。RM作为集群资源的管理和调度的角色，如果存在单点故障，则整个集群的资源都无法使用。在2.4.0版本才新增RM HA的特性，这样就增加了RM的可用性。 NodeManager（NM） NM是Yarn集群的Slave，是集群中实际拥有实际资源的工作节点。我们提交Job以后，会将组成Job的多个Task调度到对应的NM上进行执行。Hadoop集群中，为了获得分布式计算中的Locality特性，会将DN和NM在同一个节点上运行，这样对应的HDFS上的Block可能就在本地，而无需在网络间进行数据的传输。 Container Container是Yarn集群中资源的抽象，将NM上的资源进行量化，根据需要组成一个个Container，然后服务于已授权资源的计算任务。计算任务在完成计算后，系统会回收资源，以供后续计算任务申请使用。Container包含两种资源：内存和CPU，后续Hadoop版本可能会增加硬盘、网络等资源。 ApplicationMaster（AM） AM主要管理监控部署在Yarn集群上的Application，以MapReduce为例，MapReduce ApplicationMaster是一个用来处理MapReduce计算的服务框架程序，为用户编写的MapReduce程序提供运行时支持。通常我们在编写的一个MapReduce程序可能包含多个Map Task或Reduce Task，而各个Task的运行管理与监控都是由这个MapReduce ApplicationMaster来负责，比如运行Task的资源申请，由AM向RM申请；启动/停止NM上某Task的对应的Container，由AM向NM请求来完成。 Map中的shuffle 每个map有一个环形内存缓存区，用于存储map的输出。默认大小100MB（io.sort.mb属性），一旦达到阈值0.8（io.sort.spill.percent）,一个后台线程把内容溢写到（spill）磁盘的指定目录（mapred.local.dir）下的一个新建文件中。 写磁盘前，要partition，sort。如果有combiner，combine排序后数据。 等最后记录写完，合并全部文件为一个分区且排序的文件。 Reduce通过Http方式得到输出文件的特点分区的数据。 排序阶段合map输出。然后走Reduce阶段。 reduce执行完之后，写入到HDFS中。 Hadoop序列化的特点 hadoop序列化的特点： 紧凑：高效使用存储空间。 快速：读写数据的额外开销小。 可扩展：可透明地读取老格式的数据。 互操作：支持多语言的交互 Java序列化的不足： 不精简。附加信息多。不大适合随机访问。 存储空间大。递归地输出类的超类描述直到不再有超类。 扩展性差。而Writable方便用户自定义 Hadoop的序列化格式：Writable。 Writable接口 Writable接口，是根据DataInput和DataOutput实现的简单、有效的序列化对象。 MR的任意Key和value必须实现Writable接口 MR的任意key必须实现WritableComparable接口 常用的Writable实现类 Java基本类型 Writable 序列化大小（字节） 布尔型（boolean） BooleanWritable 1 字节型（byte） ByteWritable 1 整型（int） IntWritable/VIntWritable 4/1~5 浮点型（float） FloatWritable 4 长整型（long） LongWritable/VLongWritable 8/1~9 双精度浮点型（double） DoubleWritable 8 Text一般认为它等价于java.lang.String的Writable。针对UTF-8序列。 NullWritable是单例，获取实例使用NullWritable.get()。 MapReduce默认输入处理类 InputFormat 抽象类，只是定义了两个方法 FileInputFormat FileInputFormat是所有以文件作为数据源的InputFormat实现的基类，FileInputFormat保存作为Job输入的所有文件，并实现了对输入文件计算splits的方法。至于获得记录的方法是有不同的子类——TextIntFormat进行实现的。 TextInputFormat 是默认的处理类，处理普通文本文件。 把文件中每一行作为一个记录，它将每一行在文件中的起始偏移量作为key，每一行的内容作为value。 默认以\\n或回车作为一行记录。 InputSplit 在执行mapreduce之前，原始数据被分割成若干split，每个split作为一个map任务的输入。 当Hadoop处理很多小文件（文件大小小于hdfs block大小）的时候，由于FileInputFormat不会对小文件进行划分，所以每一个小文件都会被当作一个InputSplit并分配一个map任务，会有大量的map task运行，导致效率低下。 例如：一个1G的文件会被划分成8个128MB的split，并分配8个map任务处理，而10000个100kb的文件会被10000个map任务处理 map任务的数量 一个InputSplit对应一个Map task InputSplit的大小是由Math.max(minSize,Math.min(maxSize,blockSize))决定 单节点建议运行10——100个map task map task执行时长不建议低于1分钟，否则效率低 特殊：一个输入文件大小为140M，会有几个map task？1个 FileInputFormat类中的getSplits RecordReader 每一个InputSplit都有一个RecordReader，作用是把InputSplit中的数据解析成Record，即&lt;k1,v1&gt;。 在TextInputFormat中的RecordReader是LineRecordReader，每一行解析成一个&lt;k1,v1&gt;。其中，k1表示偏移量，v1表示文本内容。 MapReduce其他输入类 DBInputFormat 一般不用，实际工作中会把数据库的数据导出到hdfs上，然后再进行计算。 CombineFileInputFormat 将多个小文件合成一个split作为输入 相对于大量的小文件来说，Hadoop更适合处理少量的大文件 CombineFileInputFormat可用缓解这个问题，它是针对小文件而设计的 针对小文件的解决思路 一种是使用这个CombineFileInputFormat 另一种是使用SequenceFileFormat 实际工作中针对小文件建议使用下面方案处理 源头尽量避免产生很多小文件，因为小文件会导致namenode内存消耗过高【默认每一个文件在namenode中大致占用150字节】 源头无法避免的话，可以定时对小文件进行合并，建议合并为SequenceFile 如果上述两种方法都无法解决的话再使用这种方案。 KeyValueTextInputFormat 当输入数据的每一行是两列，并用tab分离的形式的时候，KeyValueTextInputFormat处理这种数据非常合适 1234// 如果行中有分隔符，那么分隔符前面的作为key，后面的作为value；如果行中没有分隔符，那么整行作为key，value为空job.setInputFormatClass(KeyValueTextInputFormat.class);// 默认分隔符就是制表符，可以使用其他分隔符// conf.setStrings(KeyValueLineRecordReader.KEY_VALUE_SEPERATOE,\"\\t\"); NLineInputFormat NLineInputFormat可以控制在每个split中数据的行数 1234//设置具体输入类job.setInputFormatClass(NLineInputFormat.class);// 设置每个split的行数NLineInputFormat.setNumLinesPerSplit(job,Integer.parseInt(args[2])); SequenceFileInputFormat 当输入文件格式是sequenceFile的时候，要使用SequenceFileInputFormat作为输入。 MultipleInputs 123456789101112FileSystem fileSystem = FileSystem.newInstance(conf) fileSystem.delete(new Path(outPath),true);Path path1 = new Path(inputPath1);MultipleInputs.addInputPath(job,path1,TextInputFormat.class,Mapper1.class);Path path2 = new Path(intputPath2);MultipleInputs.addInputPath(job,path2,SequenceFileInputFormat.class,Mapper2.class);job.setReduceClass(WordCountReducer.class);job.setOutputKeyClass(Text.class);job.setOutputValueClass(LongWritable.class)FileOutputFormat.setOutputPath(job,new Path(outputPath)); OutputFormat MapReduce的输出 TextOutputFormat 默认输出格式，key和value中间用tab隔开 DBOutputFormat 一般不用，实际工作中会把数据库的数据保存到hdfs上，然后再导入到db中。 SequenceFileOutputFormat 将key和value以sequenceFile格式输出。 SequenceFileAsOutputFormat 将key和value以原始二进制的格式输出。 MapFileOutputFormat 将key和value写入MapFile中。由于MapFile中的key是有序的，所以写入的时候必须保证记录时按key值顺序写入的。 MultipleOutputFormat（多路输出） 默认情况下一个reducer会产生一个输出，但是有些时候我们想一个reduce产生多个输出，MultipleOutputFormat和MultipleOutputs可以实现这个功能。 SequenceFile和MapFile 小文件存在的问题 Hadoop的HDFS和MapReduce子框架主要是针对大数据文件来设计的，再小文件的处理上不但效率低下，而且十分消耗内存资源（每个小文件占用一个Block，每一个block的元数据都存储再namenode的内存里，每个文件信息和块信息大约都要占150字节）。 解决办法通常是选择一个容器，将这些小文件组织起来统一存储。HDFS提供了两种类型的容易，分别是SequenceFile和MapFile。 SequenceFile SequenceFile是Hadoop API提供的一种二进制文件。这种二进制文件直接将&lt;key,value&gt;对序列化到文件中。一般对小文件可以使用这种文件合并，即将文件名作为key。文件内容作为value序列化到大文件中。这种文件格式有以下好处： 支持压缩，且可定制为基于Record或Block压缩（Block级压缩性能较优） 本地化任务支持：因为文件可以被切分，因此MapReduce执行任务时数据的本地化情况应该是非常好的。 对key、value的长度进行了定义，（反）序列化速度比较快 缺点时需要一个合并文件的过程，文件较大，且合并后的文件不方面查看，必须通过遍历查看每一个小文件。 MapFile MapFile是排序后的SequenceFile，通过观察其目录结构可以看到MapFile由两部分组成，分别是index和data。 index作为文件的数据索引，主要记录了每个Record的key值，以及该Record在文件中的便宜位置。在MapFile被访问的时候，索引文件会被加载到内存，通过索引映射关系可迅速定位到指定Record所在文件位置，因此，相对SequenceFile而言，MapFile的检索效率是高效的，缺点是消耗一部分内存来存储index数据。 MapReduce引用第三方jar包 第一种：将第三方jar包和你的MapReduce程序打成一个jar包。 优点：使用的时候方便，只需要指定mapreduce的jar包即可 缺点：如果依赖的jar包很多，会造成打的依赖包很大，上传服务器会比较慢 第二种： 使用libjars这个参数 Hadoop jar hello.jar packagename.className -libjars /data/fastjson-1.2.47.jar /inputpath /outputpath 优点：mapreduce的jar包中包含业务代码，打包以及上传都很快。多个依赖jar之间用逗号隔开 jar包的路径可以使用hdfs路径 缺点：每次启动jar包的时候都需要在后面指定一堆依赖的jar名称 解决方案：可以使用shell脚本保存命令 想要使用 -libjars需要调整一下代码 123456Configuration conf = new Configuration();// 处理命令行传递的参数String[] remainArgs = new GenericOptionsPaser(conf,args).getRemainArgs();// 此时这两个参数不能直接从main函数的args参数中获取，需要从remainingArgs中获取String inputPath = remainingArgs[0];String outPutPath = remainingArgs[1]; ​","link":"/2019/09/04/大数据框架Hadoop-四-MapReduce1/"},{"title":"ElasticSearch入门","text":"ElasticSearchElasticsearch简介及安装部署 ElasticSearch是一个实时的分布式的搜索和分析引擎。它是对lucene进行封装。能够达到实时搜索、稳定可靠、快速等特点。基于REST接口（API） 普通接口请求是…get?a=1 rest接口请求是…get/a/1 ElasticSearch的用户 GitHUb、WiKIpedia、eBay等。 全文检索工具 Lucene Java的一个开源搜索引擎，在java世界中是标准的全文检索程序，提供了完整的查询引擎和搜索引擎。http://lucene.apache.org/ solr（solr.4.x solrcloud） solr是一个用java开发的独立的企业级的搜索应用服务器，提供了类时于Web-service的API接口，它是基于Lucene的全文检索服务器。http://lucene.apache.org/solr/ ElasticSearch ElasticSearch是一个采用java语言开发的基于Lucene构造的开源的、分布式的搜索引擎，能够实现实时搜索。http://lucene.apache.org/solr/ MySQL和ES区别 MySQL elasticsearch database(数据库) index(索引库) table（表） type(类型) row(行) document(文档) column(列) field(字段) Elasticsearch的基本操作 curl 命令 -x 指定http请求的方法 （GET POST PUT DELETE） -d 指定要传递的参数 -H header信息 curl 命令简单使用 curl 创建索引库 curl -XPUT 'http://hadoop128:9200/test/' PUT/POST都可以 curl 创建索引 123456curl -H \"Content-Type:application/json\" -XPOST http://hadoop128:9200/test/emp/1-d '{ \"name\":\"tom\", \"age\":25}' 注意 索引库名称必须小写，不能以[_ - +]开头，也不能包含逗号 如果没有明确指定索引数据id，es会自动生成一个随机的ID(仅能使用POST参数) 1curl -H \"Content-Type:application/json\" -XPOST http://hadoop128:9200/test/emp/ -d '{ \"name\":\"tom\", \"age\":25 }' 如果想要确定我们创建的都是全新的数据 使用随机ID（POST）方式 在url后面添加参数 ​ curl -H &quot;Content-Type:application/json&quot; -XPUT http://hadoop128:9200/test/emp/2/_create -d '{ &quot;name&quot;:&quot;tom&quot;, &quot;age&quot;:25 }' curl 查询索引——GET 根据id进行查询 curl -XGET http://hadoop128:9200/test/emp/1?pretty 引号最好加上有时候可以不加，在任意的查询字符串添加pretty（pretty=true的缩写）参数，es可以得到易于识别的json结果 检索文档中的一部分，显示指定字段 curl -XGET 'http://hadoop128:9200/test/emp/1?_source=name&amp;pretty' 查询指定索引库的所有数据 curl -XGET 'http://hadoop128:9200/test/emp/_search?pretty' curl 更新 es可以使用put和post对文档进行更新（全部更新），如果指定文档已经存在，则执行更新操作 curl -XPOST -H &quot;Content-Type:application/json&quot; -XPOST http://hadoop128:9200/test/emp/1/ -d '{ &quot;stu&quot;:&quot;ac&quot;, &quot;score&quot;:20 }' 执行更新操作的时候 es首先将旧的文档标记为删除状态 添加新文档，此时旧的文档不会立即消失，但是你也无法访问 ES会在你继续添加更多数据的时候，后台字都清理已经标记为删除状态的文档 局部更新，可以添加新字段或更新已经存在的字段（必须使用POST） curl -XPOST -H &quot;Content-Type:application/json&quot; -XPOST http://hadoop128:9200/test/emp/1/_update -d '{&quot;doc&quot;:{ &quot;stu&quot;:&quot;alice&quot;}}' doc 为文档 ucrl 删除 curl -XDELETE http://hadoop128:9200/test/emp/1 如果文档存在，es会返回200 ok状态码，found属性值为true，_version属性值+1 如果文档不存在，es会返回404 not found，found属性为false,_version属性值+1，这是内部管理的一部分，保证了我们在多个节点间的不同操作顺序都被正确标记了 删除一个文档也不会立即生效，它只是被标记成已删除状态。ES会在之后添加大量数据的时候，在后台清理标记为删除状态的内容。 批量操作-bulk bulk API可以同时执行多个请求 格式 123456action:index/create/update/deletemetadata:_index,_type,_idrequest body:_source（删除操作不需要）{action:{metadata}}{request body }...... create和index的区别 如果数据存在，使用create操作失败，会提示文档已经存在，使用index则可以成功执行。 使用文件的方式 vi requests 12345678910111213{ \"index\" : { \"_index\" : \"test1\", \"_type\" : \"type1\", \"_id\" : \"1\" } }{ \"field1\" : \"value1\" }{ \"index\" : { \"_index\" : \"test1\", \"_type\" : \"type1\", \"_id\" : \"2\" } }{ \"field1\" : \"value1\" }{ \"delete\" : { \"_index\" : \"test1\", \"_type\" : \"type1\", \"_id\" : \"2\" } }{ \"create\" : { \"_index\" : \"test1\", \"_type\" : \"type1\", \"_id\" : \"3\" } }{ \"field1\" : \"value3\" }{ \"update\" : {\"_index\" : \"test1\", \"_type\" : \"type1\",\"_id\" : \"1\" } }{ \"doc\" : {\"field2\" : \"value2\"} } curl -XPOST -H &quot;Content-Type:application/json&quot; -XPOST hadoop128:9200/test1/emp/_bulk --data-binary @requests bulk 请求可以在URL中声明/_index或者 _index/ _type bulk最大处理多少数据 bulk会把将要处理的数据载入到内存中，所有数据量是有限制的 最佳的数据量不是一个确定的数值，它取决于你的硬件、你的文档大小以及复杂性，你的索引以及ES的负载 一般建议是1000-5000个文档，如果文档很大，可以适当减少队列，大小建议是5-15MB，默认不能超过100M，可以在es的配置文件中修改这个值http.max.content_length:100mb【不建议修改，太大的话bulk也会慢】 https://www.elastic.co/guide/en/elasticsearch/reference/6.4/modules-http.html Elasticsearch插件介绍 ElasticSearch Head Plugin 很方便对es进行各种操作的客户端 按照步骤参考某度和某歌 Elasticsearch配置参数详解 elasticsearch.yml es已经为大多数参数设置合理的默认值 这个文件是yml格式文件 属性顶格写，不能有空格 缩放一定不能使用tab制表符，只能使用空格 属性和值之间的：后面需要空格 network.host: 192.168.1.4 部分参数介绍 参数: 值 介绍 cluster.name: my-application 集群名称，默认是elasticsearch node.name: node-1 节点名称，如不指定 默认字段生成 path.data: /path/to/data es数据存储目录，默认存储在es_home/data目录下 path.logs: /path/to/logs es日志存储目录， 默认存储在es_home/logs目录下 bootstrap.memory_lock: true 锁定物理内存地址，防止elasticsearch内存被交换出去，也就是避免es使用swap分区 network.host: 192.168.0.1 本机ip，es1.x绑定的是0.0.0.0不需要自己配置，es2.x起默认绑定127.0.0.1，需要修改 http.port: 9200 es 服务端口 discovery.zen.ping.unicast.hosts: [“host1”, “host2”] 启动新节点，通过这个ip列表发现组件集群 discovery.zen.minimum_master_nodes: 防止集群脑裂现象 （九群总节点数量/2）+1 gateway.recover_after_nodes: 3 一个集群n个基点启动后，才允许进行数据恢复处理，默认是1 action.destructive_requires_name: true 设置是否可以通过正则或_all删除或者关闭索引库，默认true表示必须要显式指定索引库名称，生产环境建议设置为true，删除索引库时必须显式指定，否则可能会误删索引库中的索引库 Elasticsearch核心概念 cluster 代表一个集群，集群中有多个节点，其中一个为主节点，这个主节点是可以通过选举产生的，主从节点是对于集群内部来说的，从外部看es集群，在逻辑上是个整体，与任何一个节点和通信和与整个集群通信是等价的。 主节点的职责是负责管理集群状态，包括管理分片的状态和副本的状态，以及节点的发现和删除。 注意：主节点不负责对数据的增删改查请求进行处理，只负责维护集群相关状态信息。 集群配置（修改conf/elasticsearch,yml） discovery.zen.ping.unicast.hosts: [“host1”, “host2:9300”] 集群状态查看 http://hadoop128:9200/_cluster/health?pretty shards 代表索引分片，es可以把一个完整的索引分成多个分片，这样的好处是可以把一个大的索引水平拆分成多个，分布到不同的节点上。构成分布式搜索，提高性能和吞吐量。 分片的数量只能在创建索引库时指定，索引库创建后不能更改。 curl -H &quot;Content-Type:application/json&quot; -XPUT 'hadoop128:9200/test2/' -d '{&quot;settings&quot;:{&quot;number_of_shards&quot;:3}}'(此处只能用PUT) replicas 代表分片的副本，es可以给索引分片设置副本，副本的作用一是提高系统的容错性，当某个节点某个分片损坏或丢失时可以从副本中恢复。二是提高es的查询效率，es会自动对搜索请求进行负责均衡(副本数量可以随时修改) 可以在创建索引库的时候指定 curl -XPUT 'hadoop128:9200/test/' -d'{&quot;settings&quot;:{&quot;number_of_replicas&quot;:2}}' 默认一个分片有一个副本 index.number_of_replicas:1 注意：主分片和副本不会存放在一个节点中。 recovery 代表数据恢复或者叫数据重新分布，es在有节点加入或退出时会根据机器的负载对索引分片进行重新分配，挂掉的节点重新启动时也会进行数据恢复。 Elasticsearch javaapi操作 添加maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.4.3&lt;/version&gt;&lt;/dependency&gt; 连接es集群 1 通过TransportClient这个类，指定es集群中其中一台或者多台机器的ip地址和端口 1TransportClient client = new PreBuiltTransportClient(Settings.EMPTY).addTransportAddress(new TransportAddress(InetAddress.getByName(\"host1\"), 9300)).addTransportAddress(new TransportAddress(InetAddress.getByName(\"host2\"), 9300)); 2 添加其他配置 12345Settings settings = Settings.builder() .put(\"cluster.name\", \"myClusterName\") //集群名称 .put(\"client.transport.sniff\", true) //打开嗅探，es会自动把集群其他机器ip地址加到客户端 .build();TransportClient client = new PreBuiltTransportClient(settings).addTransportAddress(new TransportAddress(InetAddress.getByName(\"host1\"), 9300)).addTransportAddress(new TransportAddress(InetAddress.getByName(\"host2\"), 9300)); 3 es-java客户端操作 索引Index（json，map，bean，es helper） IndexResponse response = client.prepareIndex(&quot;test&quot;, &quot;emp&quot;, &quot;1&quot;).setSource().get() //建立索引 查询 Get GetResponse response = client.prepareGet(&quot;test&quot;, &quot;emp&quot;, &quot;1&quot;).get(); 更新update 123UpdateResponse response4 = client.prepareUpdate(index, type, \"101\") .setDoc(\"{\\\"age\\\":180}\", XContentType.JSON) .get(); 删除delete DeleteResponse response5 = client.prepareDelete(index, type, &quot;101&quot;).get(); 批量操作 12345678910111213141516171819BulkRequestBuilder bulkRequestBuilder = client.prepareBulk(); IndexRequest indexRequest = new IndexRequest(index, type);indexRequest.source(\"{\\\"name\\\":\\\"zs8888\\\"}\",XContentType.JSON); // 执行批量操作 BulkResponse bulkItemResponses = bulkRequestBuilder.get(); // 因为这一批数据在执行的时候，有可能会有一部分数据执行失败，所以在执行之后可以获取一些失败信息 if(bulkItemResponses.hasFailures()){ // 如果有失败进行，则为true // 获取失败信息 BulkItemResponse[] items = bulkItemResponses.getItems(); for (BulkItemResponse item: items) { // 打印具体的失败消息 System.out.println(item.getFailureMessage()); } }else{ System.out.println(\"全部执行成功!!!\"); } 查询类型searchType 元素 含义 query_then_fetch 查询是针对所有的块执行的，但是返回的是足够的信息，而不是文档内容（Documnet）。结果会被排序和分级，基于此，只有相关的块的文档对象会被返回。由于被取到的仅仅是这些，故返回的hit的大小正好等于指定的size。这对于有许多块的index来说是很便利的（返回结果不会有重复的。因为块被分组了）。 query_and_fetch 最原始(也可能是最快的)实现就是简单的在所有相关的shard上执行检索并返回结果。每个shard返回一定尺寸的结果。由于shard已经返回一定尺寸的hit，这种类型实际上是返回多个shard的一定尺寸的结果给调用者。 dfs_query_then_fetch 与query_then_fench相同，预期一个初始散射相伴用来更为准确的score计算分配了的term频率 dfs_query_and_fetch 与query_and_fetch相同，预期一个初始散射相伴用来更为准确的score计算分配了的term频率 - es搜索有四种 - query and fetch（速度最快）（返回N倍数据量） - query then fetch (默认的搜索方式) - DFS query and fetch - DFS query then fetch(更精确的控制搜索打分和排名) - 总结 - 从性能考虑来说，Query_AND_FETCH 是最快的，DFS_QUERY_THEN_FETCH是最慢的。 - 从搜索的准确度来说，DFS要比非DFS的准确度要高。 - 注 - DFS ，Distributed frequency Scatter，分布式词频率和分布式文档散发的缩写。 - 初始化过程：初始化散发就是在进行真正的查询之前，先把各个分片的词频率和文档频率收集一下，然后进行词搜索的时候，各分片依据全局的词频率和文档频率进行搜索和排名。如果使用DFS_QUERY_THEN_FETCH这种查询方式，效率是最低的，因为一个搜索可能要请求3次分片。但是用DFS方法搜索精度应该是最高的。Elasticsearch 分词详解(整合ik分词器,自定义词库,热更新词库) es搜索系统的索引结构是倒排索引 结构（ 单词ID：记录每个单词的单词编号 单词： 对应单词 文档频率： 代表文档集合中有多少文档包含某单词 倒排列表： 包含单词ID以及其他必要信息 Docld： 单词出现的文档id TF：单词在某个文档中出现的次数 POS: 单词在文档中出现的位置 ） 分词器 分词器是把一段文本中的词按一定规则进行切分。对应的是Analyzer类，这是一个抽象类，具体规则是由子类实现的，所以对于不同的语言，要用不同的分词器，不同语言的分词器是不同的。 在创建索引时会用到分词器，在搜索时也会用到分词器，两个地方要是使用同一个分词器，否则可能会搜索不出结果。 工作流程 1切分关键词 2去除停用词(a,an,the of，的了 ) 英文停用词 http://www.ranks.nl/stopwords 中文停用词 http://www.ranks.nl/stopwords/chinese-stopwords 3 对于英文，全部转小写（搜索时不区分大小写） 中文分词有 单词分词，二分分词，词库分词 几个重要分词器 StandardAnalyzer 单字分词 ChinessAnalyzer 单字分词 CJKAnzlyzer 二分分词 IKAnalyzer 词库分词 ES中文分词插件 es-ik es官方默认分词器 对中文分词效果不理想 集成ik分词工具 下载es的ik插件 https://github.com/medcl/elasticsearch-analysis-ik/releases 解压到es_home/plugins/ik目录下 unzip 重启es 测试分词效果 curl -H “Content-Type: application/json” ‘http://localhost:9200/test/_analyze?pretty=true' -d ‘{“text”:”我们是中国人”,”tokenizer”:”ik_max_word”}’ 分词模式： ik_max_word:会将文本做最细颗粒度的拆分，会穷尽各种可能的组合 ik_smart: 会做最粗粒度的拆分。 es-ik 自定义词库 自定义词库 在ik目录下面创建一个custom目录，创建一个my.dic文件,放入不想被切分的词 编辑config/IKAnalyzer.cfg.xml文件 &lt;entry key=&quot;ext_stopwords&quot;&gt;custom/my.dic&lt;/entry&gt; 重启es 热更新ik词库 编辑onfig/IKAnalyzer.cfg.xml文件 &lt;entry key=&quot;remote_ext_dict&quot;&gt;网络位置&lt;/entry&gt; 可以从网络中添加一个文件作为词典如http://192.168.80.100:8080/hot.dic 词典文件必须是utf8 无bom的。 Elasticsearch 查询详解(置顶查询,高亮,聚合等…)1234567891011121314151617181920212223242526272829SearchResponse searchResponse = client.prepareSearch(index) .setQuery(QueryBuilders.matchAllQuery()) // 查询所有数据 //.setQuery(QueryBuilders.matchQuery(\"name\",\"tom\")) // 根据某一列进行模糊查询，不支持通配符 // .setQuery(QueryBuilders.multiMatchQuery(\"tom\",\"name\",\"dada\"))// 根据多列进行模糊查询 // .setQuery(QueryBuilders.queryStringQuery(\"name:to*\")) //lucene语法，支持*？通配符 /* .setQuery(QueryBuilders.boolQuery() .should(QueryBuilders.matchQuery(\"name\",\"tom\")).boost(5.0f) .should(QueryBuilders.matchQuery(\"age\",45)).boost(1.0f) )*/ // .setQuery(QueryBuilders.termQuery(\"name\",\"tom\"))//默认会分词 // 默认索引库分词，termQuery查不到数据 //.setQuery(QueryBuilders.queryStringQuery(\"name:\\\"tom train\\\"\")) .setSize(5) //一次获取多少条数据，默认10 .setFrom(0) // 表示从哪一条数据开始，默认角标为0 .addSort(\"age\",SortOrder.DESC) // 指定字段进行排序 .setPostFilter(QueryBuilders.rangeQuery(\"age\").from(15).to(20))//对数据进行过滤 .setExplain(true) // 根据数据匹配度返回数据 .get(); SearchHits hits = searchResponse.getHits(); // 获取查询数据的总条数 long totalHits = hits.getTotalHits(); System.out.println(\"数据总条数\"+totalHits); for(SearchHit item:hits){ System.out.println(item.getSourceAsString()); } 统计 使用aggregations 根据字段进行分组统计 根据字段分组，统计其他字段的值 size设置为0，会获取所有数据，否则，默认返回前10个分组的数据。 1234567891011121314151617181920212223242526272829303132333435363738394041/** * 聚合，aggregation 分组求sum * @param client */ private static void testAggSum(TransportClient client) { SearchResponse searchResponse = client.prepareSearch(\"test\")// 指定索引库信息。可以指定多个，中间用逗号隔开 .setQuery(QueryBuilders.matchAllQuery())// 指定查询规则 .addAggregation(AggregationBuilders.terms(\"name_term\").field(\"name.keyword\")//默认name是Text类型，这个类型不支持排序，所以需要使用这个字段的keyword类型 .subAggregation(AggregationBuilders.sum(\"score_sum\").field(\"score\")) )// 指定分组字段和聚合字段信息 .get(); // 获取分组信息 Terms name_term = searchResponse.getAggregations().get(\"name_term\"); List&lt;? extends Terms.Bucket&gt; buckets = name_term.getBuckets(); for (Terms.Bucket bk: buckets) { // 获取分值的和 Sum score_sum = bk.getAggregations().get(\"score_sum\"); System.out.println(bk.getKey()+\"----\"+score_sum.getValue()); } }/** * 统计相同年龄学员的个数 * @param client */ private static void testAggcount(TransportClient client) { SearchResponse searchResponse = client.prepareSearch(index) .setQuery(QueryBuilders.matchAllQuery()) .addAggregation(AggregationBuilders.terms(\"age_term\").field(\"age\")) .get(); Terms age_term = searchResponse.getAggregations().get(\"age_term\"); List&lt;? extends Terms.Bucket&gt; buckets = age_term.getBuckets(); for (Terms.Bucket bk: buckets) { System.out.println(bk.getKey()+\"__\"+bk.getDocCount()); } } ES 删除索引库 curl -XDELETE ‘http://localhost:9200/test' transportClient.admin().indices().prepareDelete(“test”).get(); 注意：这样会把索引库及索引库中的所有数据都删掉，慎用。 ES 分页 与SQL使用LIMIT来控制单”页“数量类似，Elasticsearch使用的是form以及size两个参数： from：从哪条结果开始，默认值为0 size：每次返回多少个结果，默认为10 假设每页显示5条结果，1页至3页的请求就是： GET /_search?size=5 GET /_search?size=5&amp;from=5 GET /_search?size=5&amp;from=10 注意： 不要一次请求过多或者页码过大的结果，这么做会对服务器造成很大压力。因为它们会在返回前排序。一个请求会经过多个分片。每个分片都会生成自己的排序结果。然后再集中进行整理，以确保最终结果的正确性。 Elasticsearch的settings和mappings详解 settings修改索引库默认配置 如分片数量、副本数量。 查看：curl -XGET http://localhost:9200/test/_settings?pretty 操作不存在索引：curl -H &quot;Content-Type: application/json&quot; -XPUT 'localhost:9200/test1/' -d'{&quot;settings&quot;:{&quot;number_of_shards&quot;:3,&quot;number_of_replicas&quot;:0}}' 操作已经存在索引：curl -H &quot;Content-Type: application/json&quot; -XPUT 'localhost:9200/test1/_settings' -d'{&quot;index&quot;:{&quot;number_of_replicas&quot;:1}}' Mapping,就是对索引库中索引的字段名称以及数据类型进行定义，类似于mysql的表结构信息。不过es的mapping比数据库灵活很多，它可以动态识别字段。一般不需要指定mapping都可以。因为es会自动根据数据格式识别它的类型，如果你需要对某些字段添加特殊属性（定义使用其它分词器、是否分词、是否存储），就必须手动添加mapping。 查询索引库的mapping信息 curl -XGET http://localhost:9200/test/emp/_mapping?pretty 指定分词器 curl -H &quot;Content-Type: application/json&quot; -XPUT 'localhost:9200/test2' -d'{&quot;mappings&quot;:{&quot;emp&quot;:{&quot;properties&quot;:{&quot;name&quot;:{&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;}}}}}' Elasticsearch的分片查询方式 默认randomize across shards 随机读取，表示随机从分片中读取数据。 _local:指查询操作会优先在本地节点有的分片进行查询，没有的话再去其他节点查询。 _only_local:指查询只会在本地节点有的分片中查询。 _primary: 指查询只在主分片中查询。 _replica:指查询只在副本中查询 _primary_first: 指查询会先在主分片中查询，如果主分片找不到(挂了),再去副本中查询。 _replica_first: 指查询会先在副本中查询，如果副本找不到了(挂了),就会在主分片中查询。 _only_node: 指在指定id的节点里面进行查询，如果该节点只要有查询索引的部分分片，就会在这部分分片中查找，所以查询结果可能不完整。 _only_nodes: 指定多个节点id，查询多个节点的数据。 _prefer_node:nodeid 优先在指定的节点上查询 _shards:0,1,2,3：查询指定分片的数据。 Elasticsearch的脑裂问题分析 同一个集群中的不同节点对集群的状态有了不一样的理解。 discovery.zen.minimum_master_nodes 用于控制选举行为发生的最小集群节点数量。推荐设为大于1的数值，因为只有在2个以上节点的集群中，主节点才是有意义的。 Elasticsearch扩展之索引模板和索引别名 在实际工作中针对一批大量数据存储的时候需要使用多个索引库，如果手工指定每个索引库的配置信息(settings和mappings)的话就很麻烦了。 创建模板 https://www.elastic.co/guide/en/elasticsearch/reference/6.4/indices-templates.html 12345678910111213141516171819202122232425262728curl -H \"Content-Type: application/json\" -XPUT localhost:9200/_template/template_1 -d '{ \"template\" : \"*\", \"order\" : 0, \"settings\" : { \"number_of_shards\" : 1 }, \"mappings\" : { \"type1\" : { \"_source\" : { \"enabled\" : false } } }}'curl -XPUT localhost:9200/_template/template_2 -d '{ \"template\" : \"te*\", \"order\" : 1, \"settings\" : { \"number_of_shards\" : 1 }, \"mappings\" : { \"type1\" : { \"_source\" : { \"enabled\" : true } } }}' 注意 order值大的模板内容会覆盖order值小的。 查看模板： curl -XGET localhost:9200/_template/temp*?pretty 删除模板： curl -XDELETE localhost:9200/_template/temp_1 ES扩展之index alias 索引别名的应用场景： 公司使用es收集应用的运行日志，每个星期创建一个索引库，这样时间长了就会创建很多的索引库，操作和管理的时候很不方便。 由于新增索引数据只会操作当前这个星期的索引库，所以就创建了两个别名 curr_week：这个别名指向这个星期的索引库，新增数据操作这个索引 last_3_month：这个别名指向最近三个月的所有索引库，因为我们的需求是查询最近三个月的日志信息。后期只需要修改这两个别名和索引库之间的指向关系即可。应用层代码不需要任何改动。还要把三个月以前的索引库close掉，留存最近一年的日志数据，一年以前的数据删除掉。 ES默认对查询的索引的分片总数量有限制，默认是1000个，使用通配符查询多个索引库的时候会这个问题，通过别名可以解决这个问题 Elasticsearch参数优化优化1 解决启动的警告信息 max file descriptors [4096] for elasticsearch process likely too low, consider increasing to at least [65536] vi /etc/security/limits.conf 添加下面两行(使用root用户) * soft nofile 65536 * hard nofile 131071 修改配置文件调整ES的JVM内存大小 修改bin/elasticsearch.in.sh中ES_MIN_MEM和ES_MAX_MEM的大小，建议设置一样大，避免频繁分配内存，根据服务器内存大小，一般分配60%左右（默认256M） 内存最大不要超过32G 一旦你越过这个神奇的32 GB边界，指针会切换回普通对象指针.。每个指针的大小增加，使用更多的CPU内存带宽。事实上，你使用40~50G的内存和使用32G的内存效果是一样的。 设置memory_lock来锁定进程的物理内存地址 避免交换(swapped)，来提高性能 修改文件conf/elasticsearch.yml bootstrap.memory_lock:true 需要根据es启动日志修改/etc/security/limits.conf文件(重启系统) 优化2 分片多的话，可以提升索引的能力，5-20个比较合适 如果分片数过少或过多，都会导致检索比较慢。 分片数过多会导致检索时打开比较多的文件，另外也会导致多台服务器之间大量通讯。 而分片数量过少导致单个分片索引过大，所以检索速度也会慢。 建议单个分片存储20G左右的索引数据【最高也不要超过50G,否则性能会很差】，所以分片数量=数据总量/20G 副本过多的话，可以提升搜索的能力，但是如果设置很多副本的话，也会对服务器造成额外的压力，因为主分片需要给所有副本同步数据。所以建议最多设置1-2个即可。 查看索引库某个分片占用磁盘空间大小 ​ curl -XGET &quot;localhost:9200/_cat/segments/test?v&amp;h=shard,segment,size&quot; ES 优化3 要定时对索引进行合并优化，不然segment越多，占用的segment memory越多，查询的性能也越差 索引量不是很大的情况下可以将segment设置为1 es2.1.0以前调用_optimize接口，后期改为_forcemerge接口 curl -XPOST 'http://localhost:9200/test/_forcemerge?max_num_segments=1' client.admin().indices().prepareForceMerge(&quot;test&quot;).setMaxNumSegments(1).get(); 索引合并是针对分片的。segment设置为1，则每个分片都有一个索引片段。 针对不使用的index，建议close，减少内存占用。因为之一索引处于open状态，索引库中的segement就会占用内存，close之后就只会占用磁盘空间了。 curl -XPOST 'localhost:9200/test/_close' ES 优化4 删除文档： 在es中删除文档，数据不会马上在硬盘中除去，而是在es索引中产生一个.del的文件，而在检索中这部分数据也会参与检索，es在检索过程会判断是否删除了，如果删除了再过滤掉。这样会降低检索效率。所以可以执行清除删除文档 curl -XPOST 'http://localhost:9200/test/_forcemerge?only_expunge_deletes=true' client.admin().indices().prepareForceMerge(&quot;test&quot;).setOnlyExpungeDeletes(true).get(); ES 优化5 如果在项目开始的时候需要批量入库大量数据的话，建议将副本数设置为0 因为es在索引数据的时候，如果有副本存在，数据包也会马上同步到副本中，这样会对es增加压力。可以等索引完成后将副本按需要改回来，这样可以提高索引效率。 ES需要注意的问题使用java操作es集群的时候要保证本地使用的es的版本和集群上es的版本保持一致。 ​ es集群的jdk版本不一致可能会导致 org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream Elasticsearch源码分析 elasticsearch在建立索引时，根据id或(id，类型)进行hash，得到hash值之后再与该索引的分片数量取模，取模的值即为存入的分片编号。 可以指定把数据存储到某一个分片中，通过routing参数 curl -XPOST 'localhost:9200/yehua/emp?routing=rout_param' -d '{&quot;name&quot;:&quot;zs&quot;,&quot;age&quot;:20}' routing（路由参数） 注意： 显著提高查询性能，routing，(急速查询)","link":"/2019/01/29/ElasticSearch入门/"}],"tags":[{"name":"大数据","slug":"大数据","link":"/tags/大数据/"},{"name":"Logstash","slug":"Logstash","link":"/tags/Logstash/"},{"name":"ElasicSearch","slug":"ElasicSearch","link":"/tags/ElasicSearch/"},{"name":"Kibana","slug":"Kibana","link":"/tags/Kibana/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"shell交互","slug":"shell交互","link":"/tags/shell交互/"},{"name":"Mysql","slug":"Mysql","link":"/tags/Mysql/"},{"name":"内连接与外连接","slug":"内连接与外连接","link":"/tags/内连接与外连接/"},{"name":"文件读写","slug":"文件读写","link":"/tags/文件读写/"},{"name":"impala","slug":"impala","link":"/tags/impala/"},{"name":"数据仓库","slug":"数据仓库","link":"/tags/数据仓库/"},{"name":"Spark优化","slug":"Spark优化","link":"/tags/Spark优化/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"网络接口","slug":"网络接口","link":"/tags/网络接口/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"},{"name":"分布式管理","slug":"分布式管理","link":"/tags/分布式管理/"},{"name":"Haoop","slug":"Haoop","link":"/tags/Haoop/"},{"name":"HDFS","slug":"HDFS","link":"/tags/HDFS/"},{"name":"Flink","slug":"Flink","link":"/tags/Flink/"},{"name":"流式处理和批处理","slug":"流式处理和批处理","link":"/tags/流式处理和批处理/"},{"name":"Hadoop，Spark, Storm,Hive, Hbase,Redis","slug":"Hadoop，Spark-Storm-Hive-Hbase-Redis","link":"/tags/Hadoop，Spark-Storm-Hive-Hbase-Redis/"},{"name":"Flume","slug":"Flume","link":"/tags/Flume/"},{"name":"Zookeeper","slug":"Zookeeper","link":"/tags/Zookeeper/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"Sqoop","slug":"Sqoop","link":"/tags/Sqoop/"},{"name":"Hive","slug":"Hive","link":"/tags/Hive/"},{"name":"数据库","slug":"数据库","link":"/tags/数据库/"},{"name":"数据库原理","slug":"数据库原理","link":"/tags/数据库原理/"},{"name":"数据库范式","slug":"数据库范式","link":"/tags/数据库范式/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"MapReduce","slug":"MapReduce","link":"/tags/MapReduce/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"搜索引擎","slug":"搜索引擎","link":"/tags/搜索引擎/"}],"categories":[{"name":"python","slug":"python","link":"/categories/python/"},{"name":"数据库原理","slug":"数据库原理","link":"/categories/数据库原理/"},{"name":"impala","slug":"impala","link":"/categories/impala/"},{"name":"Hadoop","slug":"Hadoop","link":"/categories/Hadoop/"},{"name":"大数据","slug":"大数据","link":"/categories/大数据/"}]}